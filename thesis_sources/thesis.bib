
@inproceedings{adamoReinforcementLearningAndroid2018,
  title = {Reinforcement Learning for {{Android GUI}} Testing},
  booktitle = {Proceedings of the 9th {{ACM SIGSOFT International Workshop}} on {{Automating TEST Case Design}}, {{Selection}}, and {{Evaluation}}},
  author = {Adamo, David and Khan, Md Khorrom and Koppula, Sreedevi and Bryce, Renée},
  date = {2018-11-05},
  pages = {2--8},
  publisher = {{ACM}},
  location = {{Lake Buena Vista FL USA}},
  doi = {10.1145/3278186.3278187},
  abstract = {This paper presents a reinforcement learning approach to automated GUI testing of Android apps. We use a test generation algorithm based on Q-learning to systematically select events and explore the GUI of an application under test without requiring a preexisting abstract model. We empirically evaluate the algorithm on eight Android applications and find that the proposed approach generates test suites that achieve between 3.31\% to 18.83\% better block-level code coverage than random test generation.},
  eventtitle = {{{ESEC}}/{{FSE}} '18: 26th {{ACM Joint European Software Engineering Conference}} and {{Symposium}} on the {{Foundations}} of {{Software Engineering}}},
  isbn = {978-1-4503-6053-1},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\GBEDITZ6\\Adamo et al. - 2018 - Reinforcement learning for Android GUI testing.pdf}
}

@article{agustssonExtremeLearnedImage,
  title = {Extreme {{Learned Image Compression}} with {{GANs}}},
  author = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Gool, Luc Van},
  pages = {4},
  abstract = {We propose a framework for extreme learned image compression based on Generative Adversarial Networks (GANs), obtaining visually pleasing images at significantly lower bitrates than previous methods. This is made possible through our GAN formulation of learned compression combined with a generator/decoder which operates on the fullresolution image and is trained in combination with a multiscale discriminator. Additionally, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from a semantic label map extracted from the original image, therefore only requiring the storage of the preserved region and the semantic label map. A user study confirms that for low bitrates, our approach significantly outperforms state-of-the-art methods, saving up to 67\% compared to the next-best method BPG.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\KTV4MTSZ\\Agustsson et al. - Extreme Learned Image Compression with GANs.pdf}
}

@inproceedings{agustssonGenerativeAdversarialNetworks2019,
  title = {Generative {{Adversarial Networks}} for {{Extreme Learned Image Compression}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Van Gool, Luc},
  date = {2019-10},
  pages = {221--231},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00031},
  abstract = {We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\FB4QZJ4F\\Agustsson et al. - 2019 - Generative Adversarial Networks for Extreme Learne.pdf}
}

@inproceedings{akbarContinuesCodeRecommendation2020,
  title = {Towards Continues Code Recommendation and Implementation System: An Initial Framework},
  shorttitle = {Towards Continues Code Recommendation and Implementation System},
  booktitle = {Proceedings of the {{Evaluation}} and {{Assessment}} in {{Software Engineering}}},
  author = {Akbar, Muhammad Azeem and Huang, Zhiqiu and Yu, Zhou and Mehmood, Faisal and Hussain, Yasir and Hamza, Muhammad},
  date = {2020-04-15},
  pages = {439--444},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3383219.3383282},
  abstract = {In the current era, the auto and reliable recommendation system plays a significant role in human life. The code recommender systems are being used in various source code databases to recommend the most suitable source code to the user. While code recommendation, the code analysis concerning 'code quality' and 'code implementation' is important to recommend the most reliable code by considering the objective of the user. The ultimate aim of this research work is to propose a code recommendation and implementation model using the characteristics of DevOps that assist in extracting, analyzing, implementing, and updating the recommender system continuously. The current study presents an initial framework of the proposed code recommender model. The design of the model is based on the data collected through literature review and by conducting an empirical study with experts. We believe that the proposed model will assist the researchers and practitioners to recommend the most secure and suitable source code according to their requirement.},
  isbn = {978-1-4503-7731-7},
  keywords = {Code recommendation system,DevOps,Empirical investigation},
  file = {C\:\\Users\\felix\\Zotero\\storage\\PKSP4YUY\\Akbar et al. - 2020 - Towards continues code recommendation and implemen.pdf}
}

@inproceedings{alvernazAutoencoderaugmentedNeuroevolutionVisual2017,
  title = {Autoencoder-Augmented Neuroevolution for Visual Doom Playing},
  booktitle = {2017 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Alvernaz, Samuel and Togelius, Julian},
  date = {2017-08},
  pages = {1--8},
  issn = {2325-4289},
  doi = {10.1109/CIG.2017.8080408},
  abstract = {Neuroevolution has proven effective at many re-inforcement learning tasks, including tasks with incomplete information and delayed rewards, but does not seem to scale well to high-dimensional controller representations, which are needed for tasks where the input is raw pixel data. We propose a novel method where we train an autoencoder to create a comparatively low-dimensional representation of the environment observation, and then use CMA-ES to train neural network controllers acting on this input data. As the behavior of the agent changes the nature of the input data, the autoencoder training progresses throughout evolution. We test this method in the VizDoom environment built on the classic FPS Doom, where it performs well on a health-pack gathering task.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  keywords = {Evolutionary computation,Games,Image reconstruction,Neural networks,Training,Visualization},
  file = {C\:\\Users\\felix\\Zotero\\storage\\GFM6K9UN\\Alvernaz und Togelius - 2017 - Autoencoder-augmented neuroevolution for visual do.pdf;C\:\\Users\\felix\\Zotero\\storage\\2J7NVPZF\\8080408.html}
}

@online{alvernazAutoencoderaugmentedNeuroevolutionVisual2017a,
  title = {Autoencoder-Augmented {{Neuroevolution}} for {{Visual Doom Playing}}},
  author = {Alvernaz, Samuel and Togelius, Julian},
  date = {2017-07-12},
  eprint = {1707.03902},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1707.03902},
  urldate = {2021-05-26},
  abstract = {Neuroevolution has proven effective at many reinforcement learning tasks, including tasks with incomplete information and delayed rewards, but does not seem to scale well to high-dimensional controller representations, which are needed for tasks where the input is raw pixel data. We propose a novel method where we train an autoencoder to create a comparatively low-dimensional representation of the environment observation, and then use CMA-ES to train neural network controllers acting on this input data. As the behavior of the agent changes the nature of the input data, the autoencoder training progresses throughout evolution. We test this method in the VizDoom environment built on the classic FPS Doom, where it performs well on a health-pack gathering task.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\Y5P2H6UN\\Alvernaz und Togelius - 2017 - Autoencoder-augmented Neuroevolution for Visual Do.pdf}
}

@software{antoyoAntoyoRelm2017,
  title = {Antoyo/Relm},
  author = {{antoyo}},
  origdate = {2017-02-10T03:45:15Z},
  url = {https://github.com/antoyo/relm},
  urldate = {2021-12-13},
  abstract = {Idiomatic, GTK+-based, GUI library, inspired by Elm, written in Rust},
  keywords = {hacktoberfest}
}

@online{AreWeGUI,
  title = {Are We {{GUI}} Yet?},
  url = {https://www.areweguiyet.com/},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5XA5GIAH\\www.areweguiyet.com.html}
}

@incollection{asadiEncoderDecoderFrameworkIts2020,
  title = {The {{Encoder-Decoder Framework}} and {{Its Applications}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Asadi, Ahmad and Safabakhsh, Reza},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {133--167},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_5},
  abstract = {The neural encoder-decoder framework has advanced the state-of-the-art in machine translation significantly. Many researchers in recent years have employed the encoder-decoder based models to solve sophisticated tasks such as image/video captioning, textual/visual question answering, and text summarization. In this work we study the baseline encoder-decoder framework in machine translation and take a brief look at the encoder structures proposed to cope with the difficulties of feature extraction. Furthermore, an empirical study of solutions to enable decoders to generate richer fine-grained output sentences is provided. Finally, the attention mechanism which is a technique to cope with long-term dependencies and to improve the encoder-decoder performance on sophisticated tasks is studied.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Attention mechanism,Encoder-decoder framework,Image captioning,Long-term dependencies,Machine translation,Question answering,Video caption generation},
  file = {C\:\\Users\\felix\\Zotero\\storage\\EDXS8XXI\\Asadi und Safabakhsh - 2020 - The Encoder-Decoder Framework and Its Applications.pdf}
}

@article{bansiyaHierarchicalModelObjectoriented2002,
  title = {A Hierarchical Model for Object-Oriented Design Quality Assessment},
  author = {Bansiya, J. and Davis, C. G.},
  date = {2002-01},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {28},
  number = {1},
  pages = {4--17},
  issn = {1939-3520},
  doi = {10.1109/32.979986},
  abstract = {The paper describes an improved hierarchical model for the assessment of high-level design quality attributes in object-oriented designs. In this model, structural and behavioral design properties of classes, objects, and their relationships are evaluated using a suite of object-oriented design metrics. This model relates design properties such as encapsulation, modularity, coupling, and cohesion to high-level quality attributes such as reusability, flexibility, and complexity using empirical and anecdotal information. The relationship or links from design properties to quality attributes are weighted in accordance with their influence and importance. The model is validated by using empirical and expert opinion to compare with the model results on several large commercial object-oriented systems. A key attribute of the model is that it can be easily modified to include different relationships and weights, thus providing a practical quality assessment tool adaptable to a variety of demands.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {behavioral design properties,cohesion,commercial object-oriented systems,coupling,data encapsulation,design properties,encapsulation,expert opinion,hierarchical model,high-level design quality attributes,high-level quality attributes,modularity,Object oriented modeling,object-oriented design metrics,object-oriented design quality assessment,object-oriented programming,product metrics,Quality assessment,quality assessment tool,quality attributes,reusability,software metrics,software quality},
  file = {C\:\\Users\\felix\\Zotero\\storage\\PFYQTYQ5\\979986.html}
}

@article{basiliMethodologyCollectingValid1984,
  title = {A {{Methodology}} for {{Collecting Valid Software Engineering Data}}},
  author = {Basili, Victor R. and Weiss, David M.},
  date = {1984-11},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {SE-10},
  number = {6},
  pages = {728--738},
  issn = {1939-3520},
  doi = {10.1109/TSE.1984.5010301},
  abstract = {An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Application software,Control systems,Data collection,data collection methodology,Data engineering,error analysis,error classification,Error correction,Error correction codes,Laboratories,Programming,Software engineering,software engineering experimentation,Software maintenance,Software testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\NRXWF7YQ\\Basili und Weiss - 1984 - A Methodology for Collecting Valid Software Engine.pdf;C\:\\Users\\felix\\Zotero\\storage\\PFYK3VJX\\5010301.html}
}

@article{basiliMethodologyCollectingValid1984a,
  title = {A {{Methodology}} for {{Collecting Valid Software Engineering Data}}},
  author = {Basili, Victor R. and Weiss, David M.},
  date = {1984-11},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {SE-10},
  number = {6},
  pages = {728--738},
  issn = {1939-3520},
  doi = {10.1109/TSE.1984.5010301},
  abstract = {An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.},
  eventtitle = {{{IEEE Transactions}} on {{Software Engineering}}},
  keywords = {Application software,Control systems,Data collection,data collection methodology,Data engineering,error analysis,error classification,Error correction,Error correction codes,Laboratories,Programming,Software engineering,software engineering experimentation,Software maintenance,Software testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\33P6YGBN\\Basili und Weiss - 1984 - A Methodology for Collecting Valid Software Engine.pdf;C\:\\Users\\felix\\Zotero\\storage\\WZ2E2SHW\\5010301.html}
}

@inproceedings{bauersfeldGUITestJavaLibrary2012,
  title = {{{GUITest}}: A {{Java}} Library for Fully Automated {{GUI}} Robustness Testing},
  shorttitle = {{{GUITest}}},
  booktitle = {Proceedings of the 27th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} - {{ASE}} 2012},
  author = {Bauersfeld, Sebastian and Vos, Tanja E. J.},
  date = {2012},
  pages = {330},
  publisher = {{ACM Press}},
  location = {{Essen, Germany}},
  doi = {10.1145/2351676.2351739},
  abstract = {Graphical User Interfaces (GUIs) are substantial parts of today’s applications, no matter whether these run on tablets, smartphones or desktop platforms. Since the GUI is often the only component that humans interact with, it demands for thorough testing to ensure an efficient and satisfactory user experience. Being the glue between almost all of an application’s components, GUIs also lend themselves for system level testing. However, GUI testing is inherently difficult and often involves great manual labor, even with modern tools which promise automation. This paper introduces a Java library called GUITest, which allows to generate fully automated GUI robustness tests for complex applications, without the need to manually generate models or input sequences. We will explain how it operates and present first results on its applicability and effectivity during a test involving Microsoft Word.},
  eventtitle = {The 27th {{IEEE}}/{{ACM International Conference}}},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\QBRRC57R\\Bauersfeld und Vos - 2012 - GUITest a Java library for fully automated GUI ro.pdf}
}

@report{bauersfeldUserInterfaceLevel,
  title = {User Interface Level Testing with {{TESTAR}}; What about More Sophisticated Action Specification and Selection?},
  author = {Bauersfeld, Sebastian and Vos, Tanja E J},
  url = {https://testar.org/wp-content/uploads/2015/06/testar_pub_sattose2014.pdf},
  abstract = {Testing software applications at the Graphical User Interface (GUI) level is a very important testing phase to ensure realistic tests because the GUI represents a central juncture in the application under test from where all the functionality is accessed. In earlier works we presented the TESTAR tool, a Model-Based approach to automate testing of applications at the GUI level whose objective is to generate test cases based on a model that is automatically derived from the GUI through the accessibility API. Once the model has been created, TESTAR derives the sets of visible and unblocked actions that are possible for each state that the GUI is in and randomly selects and executes actions in order to drive the tests. This paper, instead of random selection, we propose a more advanced action specification and selection mechanism developed on top of our test framework TESTAR. Instead of selecting random clicks and keystrokes that are visible and unblocked in a certain state, the tool uses a Prolog specification to derive sensible and sophisticated actions. In addition, it employs a well-known machine learning algorithm, called Q-Learning, in order to systematically explore even large and complex GUIs. This paper explains how it operates and present the results of experiments with a set of popular desktop applications.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\LPIRVWRD\\Bauersfeld und Vos - User interface level testing with TESTAR\; what abo.pdf}
}

@inproceedings{bavotaRecommendingRefactoringsBased2014,
  title = {Recommending Refactorings Based on Team Co-Maintenance Patterns},
  booktitle = {Proceedings of the 29th {{ACM}}/{{IEEE}} International Conference on {{Automated}} Software Engineering},
  author = {Bavota, Gabriele and Panichella, Sebastiano and Tsantalis, Nikolaos and Di Penta, Massimiliano and Oliveto, Rocco and Canfora, Gerardo},
  date = {2014-09-15},
  series = {{{ASE}} '14},
  pages = {337--342},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2642937.2642948},
  abstract = {Refactoring aims at restructuring existing source code when undisciplined development activities have deteriorated its comprehensibility and maintainability. There exist various approaches for suggesting refactoring opportunities, based on different sources of information, e.g., structural, semantic, and historical. In this paper we claim that an additional source of information for identifying refactoring opportunities, sometimes orthogonal to the ones mentioned above, is team development activity. When the activity of a team working on common modules is not aligned with the current design structure of a system, it would be possible to recommend appropriate refactoring operations---e.g., extract class/method/package---to adjust the design according to the teams' activity patterns. Results of a preliminary study---conducted in the context of extract class refactoring---show the feasibility of the approach, and also suggest that this new refactoring dimension can be complemented with others to build better refactoring recommendation tools.},
  isbn = {978-1-4503-3013-8},
  keywords = {developers,refactoring,teams},
  file = {C\:\\Users\\felix\\Zotero\\storage\\XE679BQP\\Bavota et al. - 2014 - Recommending refactorings based on team co-mainten.pdf}
}

@inproceedings{bavotaSupportingExtractClass2012,
  title = {Supporting Extract Class Refactoring in {{Eclipse}}: The {{ARIES}} Project},
  shorttitle = {Supporting Extract Class Refactoring in Eclipse},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Bavota, G. and Lucia, A. De and Marcus, A. and Oliveto, R. and Palomba, F.},
  date = {2012-06},
  pages = {1419--1422},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227233},
  abstract = {During software evolution changes are inevitable. These changes may lead to design erosion and the introduction of inadequate design solutions, such as design antipatterns. Several empirical studies provide evidence that the presence of antipatterns is generally associated with lower productivity, greater rework, and more significant design efforts for developers. In order to improve the quality and remove antipatterns, refactoring operations are needed. In this demo, we present the Extract class features of ARIES (Automated Refactoring In EclipSe), an Eclipse plug-in that supports the software engineer in removing the “Blob” antipattern.},
  eventtitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {ARIES project,automated refactoring In EclipSe,Blob antipattern,Clustering algorithms,Couplings,Databases,Design,design antipatterns,Eclipse plug-in,Education,extract class refactoring,Feature extraction,Measurement,object-oriented methods,Quality,Refactoring,Software,software evolution,software maintenance,software quality},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SJGY22WF\\Bavota et al. - 2012 - Supporting extract class refactoring in Eclipse T.pdf;C\:\\Users\\felix\\Zotero\\storage\\ZEY8KWIM\\6227233.html}
}

@online{BCEWithLogitsLossPyTorch10,
  title = {{{BCEWithLogitsLoss}} — {{PyTorch}} 1.10.0 Documentation},
  url = {https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html},
  urldate = {2021-11-10},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CJ96T3FN\\torch.nn.BCEWithLogitsLoss.html}
}

@software{Beschreibung2021,
  title = {Beschreibung},
  date = {2021-08-02T12:44:01Z},
  origdate = {2020-04-14T11:21:08Z},
  url = {https://github.com/neuroevolution-ai/AutoEncoder},
  urldate = {2021-09-03},
  organization = {{NeuroEvolution A.I.}}
}

@online{blasiusHiDifficultSay,
  title = {Hi, Is Difficult to Say. {{At}} the Moment the Progress Is Slow, Because {{I}} Have to Decouple Some Things in the Code. {{It}} Could Be Weeks, but Maybe Also Months},
  author = {Blasius, Florian},
  url = {https://chat.redox-os.org/redox/pl/whtgsr4oztdnjgahzte8e8jz8c},
  urldate = {2021-12-13}
}

@online{blasiusMomentItIt,
  title = {At the moment it’s it possible. But after the current refactoring it will be possible.},
  author = {Blasius, Florian},
  url = {https://chat.redox-os.org/redox/pl/ox6wmmtp6prqxgmdibbh6g59na},
  urldate = {2021-12-13},
  langid = {german}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\felix\\Zotero\\storage\\S28X8P8Y\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;C\:\\Users\\felix\\Zotero\\storage\\E7VHIP3Q\\2005.html}
}

@software{caricioRafaelcaricioLvglrs2020,
  title = {Rafaelcaricio/Lvgl-Rs},
  author = {Carício, Rafael},
  origdate = {2020-04-10T13:00:20Z},
  url = {https://github.com/rafaelcaricio/lvgl-rs},
  urldate = {2021-12-13},
  abstract = {LittlevGL bindings for Rust. A powerful and easy-to-use embedded GUI with many widgets, advanced visual effects (opacity, antialiasing, animations) and low memory requirements (16K RAM, 64K Flash).}
}

@incollection{chenUsingConvolutionalNeural2020,
  title = {Using {{Convolutional Neural Networks}} to {{Forecast Sporting Event Results}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Chen, Mu-Yen and Chen, Ting-Hsuan and Lin, Shu-Hong},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {269--285},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_9},
  abstract = {Sporting events like the FIFA World Cup and the World Baseball Classic have increased in popularity, and the enthusiasm with which these competitions are reported and commented on is evidence of their wide-reaching influence. These games are popular discussion topics. Many who follow sports are only casual fans, but even these “temporary” fans have the same expectations that all fans do: the teams they support should be able to win. This study selects the National Basketball Association (NBA) to represent competitive ball sports. Predictions herein are based on the examination and statistical analysis of previous records of NBA games. This research integrates the field of sports outcome predictions with the methodology of deep learning via convolutional neural networks. Training and predictions are modelled on statistics gleaned from a total of 4,235 games over the past three years. We analyze the training results of a variety of model structures. While previous studies have applied convolutional neural networks to image or object recognition, our study proposes a specific encoding method that is integrated with deep learning in order to predict the results of future games. The prediction accuracy rate of the model herein is approximately 91\%, while the deviation is approximately 0.2. These strong results confirm the validity of our designated encoding method.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Convolutional neural networks,Deep learning,National basketball association (NBA),Sports prediction},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CX2UCTWG\\Chen et al. - 2020 - Using Convolutional Neural Networks to Forecast Sp.pdf}
}

@article{choiGuidedGUITesting2013,
  title = {Guided {{GUI}} Testing of Android Apps with Minimal Restart and Approximate Learning},
  author = {Choi, Wontae and Necula, George and Sen, Koushik},
  date = {2013-10-29},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {48},
  number = {10},
  pages = {623--640},
  doi = {10.1145/2544173.2509552},
  abstract = {Smartphones and tablets with rich graphical user interfaces (GUI) are becoming increasingly popular. Hundreds of thousands of specialized applications, called apps, are available for such mobile platforms. Manual testing is the most popular technique for testing graphical user interfaces of such apps. Manual testing is often tedious and error-prone. In this paper, we propose an automated technique, called Swift-Hand, for generating sequences of test inputs for Android apps. The technique uses machine learning to learn a model of the app during testing, uses the learned model to generate user inputs that visit unexplored states of the app, and uses the execution of the app on the generated inputs to refine the model. A key feature of the testing algorithm is that it avoids restarting the app, which is a significantly more expensive operation than executing the app on a sequence of inputs. An important insight behind our testing algorithm is that we do not need to learn a precise model of an app, which is often computationally intensive, if our goal is to simply guide test execution into unexplored parts of the state space. We have implemented our testing algorithm in a publicly available tool for Android apps written in Java. Our experimental results show that we can achieve significantly better coverage than traditional random testing and L*-based testing in a given time budget. Our algorithm also reaches peak coverage faster than both random and L*-based testing.},
  keywords = {android,automata,gui testing,learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CPCJFFK3\\Choi et al. - 2013 - Guided GUI testing of android apps with minimal re.pdf}
}

@inproceedings{choudharyAutomatedTestInput2015,
  title = {Automated {{Test Input Generation}} for {{Android}}: {{Are We There Yet}}? ({{E}})},
  shorttitle = {Automated {{Test Input Generation}} for {{Android}}},
  booktitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
  date = {2015-11},
  pages = {429--440},
  doi = {10.1109/ASE.2015.89},
  abstract = {Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
  eventtitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  keywords = {Android apps,Androids,automated testing,Humanoid robots,Java,Receivers,Software,Systematics,Test input generation,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\YGR54IQA\\Choudhary et al. - 2015 - Automated Test Input Generation for Android Are W.pdf;C\:\\Users\\felix\\Zotero\\storage\\RYH8R8UH\\7372031.html}
}

@inproceedings{choudharyAutomatedTestInput2015a,
  title = {Automated {{Test Input Generation}} for {{Android}}: {{Are We There Yet}}? ({{E}})},
  shorttitle = {Automated {{Test Input Generation}} for {{Android}}},
  booktitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
  date = {2015-11},
  pages = {429--440},
  doi = {10.1109/ASE.2015.89},
  abstract = {Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
  eventtitle = {2015 30th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  keywords = {Android apps,Androids,automated testing,Humanoid robots,Java,Receivers,Software,Systematics,Test input generation,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\9AU9IKKT\\7372031.html}
}

@online{CustomizingBuildsRelease,
  title = {Customizing {{Builds}} with {{Release Profiles}} - {{The Rust Programming Language}}},
  url = {https://doc.rust-lang.org/book/ch14-01-release-profiles.html},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\G87JKX3U\\ch14-01-release-profiles.html}
}

@article{debEvolutionaryManyobjectiveOptimization2014,
  title = {An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part i: Solving Problems with Box Constraints},
  shorttitle = {An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part i},
  author = {Deb, K. and Jain, H.},
  date = {2014-08},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {18},
  number = {4},
  pages = {577--601},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2013.2281535},
  abstract = {Having developed multiobjective optimization algorithms using evolutionary optimization methods and demonstrated their niche on various practical problems involving mostly two and three objectives, there is now a growing need for developing evolutionary multiobjective optimization (EMO) algorithms for handling many-objective (having four or more objectives) optimization problems. In this paper, we recognize a few recent efforts and discuss a number of viable directions for developing a potential EMO algorithm for solving many-objective optimization problems. Thereafter, we suggest a reference-point-based many-objective evolutionary algorithm following NSGA-II framework (we call it NSGA-III) that emphasizes population members that are nondominated, yet close to a set of supplied reference points. The proposed NSGA-III is applied to a number of many-objective test problems with three to 15 objectives and compared with two versions of a recently suggested EMO algorithm (MOEA/D). While each of the two MOEA/D methods works well on different classes of problems, the proposed NSGA-III is found to produce satisfactory results on all problems considered in this paper. This paper presents results on unconstrained problems, and the sequel paper considers constrained and other specialties in handling many-objective optimization problems.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {box constraints,Educational institutions,EMO algorithms,evolutionary computation,Evolutionary computation,evolutionary many-objective optimization algorithm,evolutionary multiobjective optimization algorithms,genetic algorithms,large dimension,many-objective optimization,Many-objective optimization,many-objective optimization problems,many-objective test problems,Measurement,MOEA/D methods,multi-criterion optimization,multicriterion optimization,non-dominated sorting,nondominated sorting,NSGA-II framework,NSGA-III,Optimization,reference points,reference-point-based many-objective evolutionary algorithm,reference-point-based nondominated sorting approach,Sociology,sorting,Statistics,unconstrained problems,Vectors,Zirconium},
  file = {C\:\\Users\\felix\\Zotero\\storage\\66BVSZYK\\Deb und Jain - 2014 - An Evolutionary Many-Objective Optimization Algori.pdf;C\:\\Users\\felix\\Zotero\\storage\\BYHZVE69\\6600851.html}
}

@software{dubeGabdubeNativewindowsgui2016,
  title = {Gabdube/Native-Windows-Gui},
  author = {Dube, Gabriel},
  origdate = {2016-09-21T16:41:38Z},
  url = {https://github.com/gabdube/native-windows-gui},
  urldate = {2021-12-13},
  abstract = {A light windows GUI toolkit for rust},
  keywords = {gui,native,native-windows-gui,rust}
}

@software{ernerfeldtEmilkEgui2019,
  title = {Emilk/Egui},
  author = {Ernerfeldt, Emil},
  origdate = {2019-01-13T15:39:15Z},
  url = {https://github.com/emilk/egui},
  urldate = {2021-12-13},
  abstract = {egui: an easy-to-use immediate mode GUI in pure Rust},
  keywords = {game-development,gui,rust}
}

@incollection{ertelNeuronaleNetze2021,
  title = {Neuronale Netze},
  booktitle = {Grundkurs Künstliche Intelligenz: Eine praxisorientierte Einführung},
  author = {Ertel, Wolfgang},
  editor = {Ertel, Wolfgang},
  date = {2021},
  series = {Computational Intelligence},
  pages = {285--349},
  publisher = {{Springer Fachmedien}},
  location = {{Wiesbaden}},
  doi = {10.1007/978-3-658-32075-1_9},
  abstract = {Nach etwa 30 Jahren Forschung an neuronalen Netzen ist mit Deep Learning der Durchbruch gelungen. Mit dem Ziel, die wichtigsten Ideen hinter Deep Learning zu verstehen, starten wir bei biologischen neuronalen Netzen und dem Hopfield Modell. Dann führen wir den bis heute fundamentalen Backpropagation Algorithmus ein um schließlich Deep Learning und dessen Anwendungen darzustellen.},
  langid = {german},
  file = {C\:\\Users\\felix\\Zotero\\storage\\P53IMNJQ\\Ertel - 2021 - Neuronale Netze.pdf}
}

@software{FltkrsFltkrs2019,
  title = {Fltk-Rs/Fltk-Rs},
  origdate = {2019-10-24T16:44:19Z},
  url = {https://github.com/fltk-rs/fltk-rs},
  urldate = {2021-12-13},
  abstract = {Rust bindings for the FLTK GUI library.},
  organization = {{fltk-rs}},
  keywords = {bindings,fltk,graphics,gui,widgets}
}

@inproceedings{fokaefsJDeodorantIdentificationApplication2011,
  title = {{{JDeodorant}}: Identification and Application of Extract Class Refactorings},
  shorttitle = {{{JDeodorant}}},
  booktitle = {2011 33rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Fokaefs, M. and Tsantalis, N. and Stroulia, E. and Chatzigeorgiou, A.},
  date = {2011-05},
  pages = {1037--1039},
  issn = {1558-1225},
  doi = {10.1145/1985793.1985989},
  abstract = {Evolutionary changes in object-oriented systems can result in large, complex classes, known as "God Classes". In this paper, we present a tool, developed as part of the JDeodorant Eclipse plugin, that can recognize opportunities for extracting cohesive classes from "God Classes" and automatically apply the refactoring chosen by the developer.},
  eventtitle = {2011 33rd {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {class refactoring extraction,clustering,Clustering algorithms,Couplings,Educational institutions,God classes,Informatics,JDeodorant Eclipse plugin,Measurement,object-oriented methods,object-oriented programming,object-oriented systems,refactoring,Software,software maintenance,software reengineering,Syntactics},
  file = {C\:\\Users\\felix\\Zotero\\storage\\89LRGHA7\\Fokaefs et al. - 2011 - JDeodorant identification and application of extr.pdf;C\:\\Users\\felix\\Zotero\\storage\\HDHSHFYF\\6032586.html}
}

@inproceedings{fokaefsJDeodorantIdentificationRemoval2007,
  title = {{{JDeodorant}}: Identification and Removal of Feature Envy Bad Smells},
  shorttitle = {Jdeodorant},
  booktitle = {2007 {{IEEE International Conference}} on {{Software Maintenance}}},
  author = {Fokaefs, M. and Tsantalis, N. and Chatzigeorgiou, A.},
  date = {2007-10},
  pages = {519--520},
  issn = {1063-6773},
  doi = {10.1109/ICSM.2007.4362679},
  abstract = {In this demonstration we present an Eclipse plug-in that identifies feature envy bad smells in Java projects and resolves them by applying the appropriate move method refactorings. The main contribution is the ability to pre-evaluate the impact of all possible move refactorings on design quality and apply the most effective one.},
  eventtitle = {2007 {{IEEE International Conference}} on {{Software Maintenance}}},
  keywords = {design quality,Eclipse plug-in,feature envy bad smells,Human factors,Informatics,Java,Java projects,JDeodorant,move method refactorings,Navigation,Pressing,software quality,System-level design},
  file = {C\:\\Users\\felix\\Zotero\\storage\\PGM8WFUX\\Fokaefs et al. - 2007 - JDeodorant Identification and Removal of Feature .pdf;C\:\\Users\\felix\\Zotero\\storage\\7CKU3QUU\\4362679.html}
}

@software{GfxrsWgpu2018,
  title = {Gfx-Rs/Wgpu},
  origdate = {2018-09-13T19:18:50Z},
  url = {https://github.com/gfx-rs/wgpu},
  urldate = {2021-12-13},
  abstract = {Safe and portable GPU abstraction in Rust, implementing WebGPU API.},
  organization = {{Rust Graphics Mages}},
  keywords = {gpu,native-libraries,rust,webgpu}
}

@online{GfxrsWgpuPulse2018,
  title = {Gfx-Rs/Wgpu - {{Pulse}}},
  origdate = {2018-09-13T19:18:50Z},
  url = {https://github.com/gfx-rs/wgpu/pulse/monthly},
  urldate = {2021-12-13},
  abstract = {Native WebGPU implementation based on gfx-hal}
}

@inproceedings{ghafariFrameworkClassifyingComparing2017,
  title = {A Framework for Classifying and Comparing Source Code Recommendation Systems},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Ghafari, M. and Moradi, H.},
  date = {2017-02},
  pages = {555--556},
  doi = {10.1109/SANER.2017.7884674},
  abstract = {The use of Application Programming Interfaces (APIs) is pervasive in software systems; it makes the development of new software much easier, but remembering large APIs with sophisticated usage protocol is arduous for software developers. Code recommendation systems alleviate this burden by providing developers with a ranked list of API usages that are estimated to be most useful to their development tasks. The promise of these systems has motivated researchers to invest considerable effort to develop many of them over the past decade, yet the achievements are not evident. To assess the state of the art in code recommendation, we propose a framework for classifying and comparing these systems. We hope the framework will help the community to conduct a systematic study to gain insight into how much code recommendation has so far achieved, in both research and practice.},
  eventtitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  keywords = {API,application program interfaces,application programming interfaces,Context,Data mining,Manuals,Proposals,Prototypes,recommender systems,Software,software engineering,Software engineering,software systems,source code (software),source code recommendation systems,usage protocol},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DVEEE432\\Ghafari und Moradi - 2017 - A framework for classifying and comparing source c.pdf;C\:\\Users\\felix\\Zotero\\storage\\7QDWWGKJ\\7884674.html}
}

@book{goodfellowDeepLearningUmfassende2018,
  title = {Deep Learning: das umfassende Handbuch : Grundlagen, aktuelle Verfahren und Algorithmen, neue Forschungsansätze},
  shorttitle = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2018},
  isbn = {978-3-95845-701-0},
  langid = {german},
  keywords = {<span class="searchword">Deep</span> <span class="searchword">learning</span>,Maschinelles Lernen}
}

@book{goodfellowDeepLearningUmfassende2018a,
  title = {Deep Learning: das umfassende Handbuch : Grundlagen, aktuelle Verfahren und Algorithmen, neue Forschungsansätze / Ian Goodfellow, Yoshua Bengio, Aaron Courville},
  shorttitle = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  date = {2018},
  isbn = {978-3-95845-701-0},
  langid = {german},
  keywords = {<span class="searchword">Deep</span> <span class="searchword">learning</span>,Maschinelles Lernen}
}

@incollection{gordienkoScalingAnalysisSpecialized2020,
  title = {Scaling {{Analysis}} of {{Specialized Tensor Processing Architectures}} for {{Deep Learning Models}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Gordienko, Yuri and Kochura, Yuriy and Taran, Vlad and Gordienko, Nikita and Rokovyi, Alexandr and Alienin, Oleg and Stirenko, Sergii},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {65--99},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_3},
  abstract = {Specialized tensor processing architectures (TPA) targeted for neural network processing has attracted a lot of attention in recent years. The computing complexity of the algorithmically different components of some deep neural networks (DNNs) was considered with regard to their further use on such TPAs. To demonstrate the crucial difference between TPU and GPU computing architectures, the real computing complexity of various algorithmically different DNNs was estimated by the proposed scaling analysis of time and speedup dependencies of training and inference times as functions of batch and image sizes. The main accent was made on the widely used and algorithmically different DNNs like VGG16, ResNet50, and CapsNet on the cloud-based implementation of TPA (actually Google Cloud TPUv2). The results of performance study were demonstrated by the proposed scaling method for estimation of efficient usage of these DNNs on this infrastructure. The most important and intriguing results are the scale invariant behaviors of time and speedup dependencies which allow us to use the scaling method to predict the running training and inference times on the new specific TPAs without detailed information about their internals even. The scaling dependencies and scaling powers are different for algorithmically different DNNs (VGG16, ResNet50, CapsNet) and for architecturally different computing hardwares (GPU and TPU). These results give the precise estimation of the higher performance (throughput) of TPAs as Google TPUv2 in comparison to GPU for the large number of computations under conditions of low overhead calculations and high utilization of TPU units by means of the large image and batch sizes. In general, the usage of TPAs like Google TPUv2 is quantitatively proved to be promising tool for increasing performance of inference and training stages even, especially in the view of availability of the similar specific TPAs like TCU in Tesla V100 and Titan V provided by NVIDIA, and others.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {CapsNet,Deep learning,GPU,Inference time,MNIST,ResNet50,Scaling,Speedup,Tensor cores,Tensor processing architecture,Tensor processing unit,TPUv2,Training time,VGG16},
  file = {C\:\\Users\\felix\\Zotero\\storage\\94Q7NFSC\\Gordienko et al. - 2020 - Scaling Analysis of Specialized Tensor Processing .pdf}
}

@online{GpuwebGpuwebImplementation2017,
  title = {Gpuweb/Gpuweb - {{Implementation Status}}},
  origdate = {2017-02-10T01:17:35Z},
  url = {https://github.com/gpuweb/gpuweb/wiki/Implementation-Status},
  urldate = {2021-12-13},
  abstract = {Where the GPU for the Web work happens!}
}

@inproceedings{grossSearchbasedSystemTesting2012,
  title = {Search-Based System Testing: High Coverage, No False Alarms},
  shorttitle = {Search-Based System Testing},
  booktitle = {Proceedings of the 2012 {{International Symposium}} on {{Software Testing}} and {{Analysis}} - {{ISSTA}} 2012},
  author = {Gross, Florian and Fraser, Gordon and Zeller, Andreas},
  date = {2012},
  publisher = {{ACM Press}},
  location = {{Minneapolis, MN, USA}},
  doi = {10.1145/2338965.2336762},
  abstract = {Modern test case generation techniques can automatically achieve high code coverage. If they operate on the unit level, they run the risk of generating inputs infeasible in reality, which, when causing failures, are painful to identify and eliminate. Running a unit test generator on five open source Java programs, we found that all of the 181 reported failures were false failures—that is, indicating a problem in the generated test case rather than the program. By generating test cases at the GUI level, our EXSYST prototype can avoid such false alarms by construction. In our evaluation, it achieves higher coverage than search-based test generators at the unit level; yet, every failure can be shown to be caused by a real sequence of input events. Whenever a system interface is available, we recommend considering search-based system testing as an alternative to avoid false failures.},
  eventtitle = {The 2012 {{International Symposium}}},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\FIN6RRIT\\Gross et al. - 2012 - Search-based system testing high coverage, no fal.pdf}
}

@software{GtkrsGtk3rs2020,
  title = {Gtk-Rs/Gtk3-Rs},
  origdate = {2020-10-14T13:46:29Z},
  url = {https://github.com/gtk-rs/gtk3-rs},
  urldate = {2021-12-13},
  abstract = {Provides Rust bindings for GTK libraries},
  organization = {{gtk-rs}},
  keywords = {atk,gdk,gdkx11,gtk,gtk-rs,gtk3,gtk3-macros}
}

@inproceedings{gveroCompleteCompletionUsing2013,
  title = {Complete Completion Using Types and Weights},
  booktitle = {Proceedings of the 34th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Gvero, Tihomir and Kuncak, Viktor and Kuraj, Ivan and Piskac, Ruzica},
  date = {2013-06-16},
  series = {{{PLDI}} '13},
  pages = {27--38},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2491956.2462192},
  abstract = {Developing modern software typically involves composing functionality from existing libraries. This task is difficult because libraries may expose many methods to the developer. To help developers in such scenarios, we present a technique that synthesizes and suggests valid expressions of a given type at a given program point. As the basis of our technique we use type inhabitation for lambda calculus terms in long normal form. We introduce a succinct representation for type judgements that merges types into equivalence classes to reduce the search space, then reconstructs any desired number of solutions on demand. Furthermore, we introduce a method to rank solutions based on weights derived from a corpus of code. We implemented the algorithm and deployed it as a plugin for the Eclipse IDE for Scala. We show that the techniques we incorporated greatly increase the effectiveness of the approach. Our evaluation benchmarks are code examples from programming practice; we make them available for future comparisons.},
  isbn = {978-1-4503-2014-6},
  keywords = {code completion,program synthesis,ranking,type inhabitation,type-driven synthesis},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DYRBK9ZK\\Gvero et al. - 2013 - Complete completion using types and weights.pdf}
}

@article{hoRealWorldWeightCrossEntropyLoss2020,
  title = {The {{Real-World-Weight Cross-Entropy Loss Function}}: {{Modeling}} the {{Costs}} of {{Mislabeling}}},
  shorttitle = {The {{Real-World-Weight Cross-Entropy Loss Function}}},
  author = {Ho, Yaoshiang and Wookey, Samuel},
  date = {2020},
  journaltitle = {IEEE Access},
  volume = {8},
  pages = {4806--4813},
  doi = {10.1109/ACCESS.2019.2962617},
  abstract = {In this paper, we propose a new metric to measure goodness-of-fit for classifiers: the Real World Cost function. This metric factors in information about a real world problem, such as financial impact, that other measures like accuracy or F1 do not. This metric is also more directly interpretable for users. To optimize for this metric, we introduce the Real-World-Weight Cross-Entropy loss function, in both binary classification and single-label multiclass classification variants. Both variants allow direct input of real world costs as weights. For single-label, multiclass classification, our loss function also allows direct penalization of probabilistic false positives, weighted by label, during the training of a machine learning model. We compare the design of our loss function to the binary cross-entropy and categorical cross-entropy functions, as well as their weighted variants, to discuss the potential for improvement in handling a variety of known shortcomings of machine learning, ranging from imbalanced classes to medical diagnostic error to reinforcement of social bias. We create scenarios that emulate those issues using the MNIST data set and demonstrate empirical results of our new loss function. Finally, we discuss our intuition about why this approach works and sketch a proof based on Maximum Likelihood Estimation.},
  eventtitle = {{{IEEE Access}}},
  keywords = {class imbalance,cross-entropy,ethnic stereotypes,Machine learning,maximum likelihood estimation,Maximum likelihood estimation,Measurement,Neural networks,oversampling,Probabilistic logic,social bias,softmax,Standards,Training,undersampling},
  file = {C\:\\Users\\felix\\Zotero\\storage\\B6SC5ESY\\Ho und Wookey - 2020 - The Real-World-Weight Cross-Entropy Loss Function.pdf;C\:\\Users\\felix\\Zotero\\storage\\6AE3USWB\\8943952.html}
}

@incollection{hosseiniDeepLearningArchitectures2020,
  title = {Deep {{Learning Architectures}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Hosseini, Mohammad-Parsa and Lu, Senbao and Kamaraj, Kavin and Slowikowski, Alexander and Venkatesh, Haygreev C.},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {1--24},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_1},
  abstract = {Deep learning is one of the most widely used machine learning techniques which has achieved enormous success in applications such as anomaly detection, image detection, pattern recognition, and natural language processing. Deep learning architectures have revolutionized the analytical landscape for big data amidst wide-scale deployment of sensory networks and improved communication protocols. In this chapter, we will discuss multiple deep learning architectures and explain their underlying mathematical concepts. An up-to-date overview here presented concerns three main categories of neural networks, namely, Convolutional Neural Networks, Pretrained Unspervised Networks, and Recurrent/Recursive Neural Networks. Applications of each of these architectures in selected areas such as pattern recognition and image detection are also discussed.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Architectures,Attention,Autoencoders,CNN,DBN,Deep learning,GAN,LSTM,Recurrent networks,Recursive networks,Unsupervised networks},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SYC386ZA\\Hosseini et al. - 2020 - Deep Learning Architectures.pdf}
}

@incollection{hosseiniDeepLearningArchitectures2020a,
  title = {Deep {{Learning Architectures}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Hosseini, Mohammad-Parsa and Lu, Senbao and Kamaraj, Kavin and Slowikowski, Alexander and Venkatesh, Haygreev C.},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {1--24},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_1},
  abstract = {Deep learning is one of the most widely used machine learning techniques which has achieved enormous success in applications such as anomaly detection, image detection, pattern recognition, and natural language processing. Deep learning architectures have revolutionized the analytical landscape for big data amidst wide-scale deployment of sensory networks and improved communication protocols. In this chapter, we will discuss multiple deep learning architectures and explain their underlying mathematical concepts. An up-to-date overview here presented concerns three main categories of neural networks, namely, Convolutional Neural Networks, Pretrained Unspervised Networks, and Recurrent/Recursive Neural Networks. Applications of each of these architectures in selected areas such as pattern recognition and image detection are also discussed.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Architectures,Attention,Autoencoders,CNN,DBN,Deep learning,GAN,LSTM,Recurrent networks,Recursive networks,Unsupervised networks},
  file = {C\:\\Users\\felix\\Zotero\\storage\\X27E92UK\\Hosseini et al. - 2020 - Deep Learning Architectures.pdf}
}

@inproceedings{huImprovingConvolutionalNeural2018,
  title = {Improving {{Convolutional Neural Network Using Pseudo Derivative ReLU}}},
  booktitle = {2018 5th {{International Conference}} on {{Systems}} and {{Informatics}} ({{ICSAI}})},
  author = {Hu, Zheng and Li, Yongping and Yang, Zhiyong},
  date = {2018-11},
  pages = {283--287},
  doi = {10.1109/ICSAI.2018.8599372},
  abstract = {Rectified linear unit (ReLU) is a widely used activation function in artificial neural networks, it is considered to be an efficient active function benefit from its simplicity and nonlinearity. However, ReLU's derivative for negative inputs is zero, which can make some ReLUs inactive for essentially all inputs during the training. There are several ReLU variations for solving this problem. Comparing with ReLU, they are slightly different in form, and bring other drawbacks like more expensive in computation. In this study, pseudo derivatives were tried replacing original derivative of ReLU while ReLU itself was unchanged. The pseudo derivative was designed to alleviate the zero derivative problem and be consistent with original derivative in general. Experiments showed using pseudo derivative ReLU (PD-ReLU) could obviously improve AlexNet (a typical convolutional neural network model) in CIFAR-10 and CIFAR-100 tests. Furthermore, some empirical criteria for designing such pseudo derivatives were proposed.},
  eventtitle = {2018 5th {{International Conference}} on {{Systems}} and {{Informatics}} ({{ICSAI}})},
  keywords = {AlexNet,Backpropagation,Convergence,convolutional neural network,Convolutional neural networks,Deep learning,Kernel,Pseudo derivative,ReLU,Training},
  file = {C\:\\Users\\felix\\Zotero\\storage\\UNKVP6M2\\Hu et al. - 2018 - Improving Convolutional Neural Network Using Pseud.pdf;C\:\\Users\\felix\\Zotero\\storage\\VKNQ4NS5\\8599372.html}
}

@online{IcedZulip,
  title = {Iced - {{Zulip}}},
  url = {https://iced.zulipchat.com/},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\ECT3DX55\\iced.zulipchat.com.html}
}

@software{ImguirsImguirs2015,
  title = {Imgui-Rs/Imgui-Rs},
  origdate = {2015-08-16T17:51:04Z},
  url = {https://github.com/imgui-rs/imgui-rs},
  urldate = {2021-12-13},
  abstract = {Rust bindings for Dear ImGui},
  organization = {{imgui-rs}},
  keywords = {gui,imgui,rust,rust-library}
}

@incollection{jiangConstructingConvolutionalNeural2020,
  title = {Constructing a {{Convolutional Neural Network}} with a {{Suitable Capacity}} for a {{Semantic Segmentation Task}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Jiang, Yalong and Chi, Zheru},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {237--268},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_8},
  abstract = {Although the state-of-the-art performance has been achieved in many computer vision tasks such as image classification, object detection, saliency prediction and depth estimation, Convolutional Neural Networks (CNNs) still perform unsatisfactorily in some difficult tasks such as human parsing which is the focus of our research. The inappropriate capacity of a CNN model and insufficient training data both contribute to the failure in perceiving the semantic information of detailed regions. The feature representations learned by a high-capacity model cannot generalize to the variations in viewpoints, human poses and occlusions in real-world scenarios due to overfitting. On the other hand, the under-fitting problem prevents a low-capacity model from developing the representations which are sufficiently expressive. In this chapter, we propose an approach to estimate the complexity of a task and match the capacity of a CNN model to the complexity of a task while avoiding under-fitting and overfitting. Firstly, a novel training scheme is proposed to fully explore the potential of low-capacity CNN models. The scheme outperforms existing end-to-end training schemes and enables low-capacity models to outperform models with higher capacity. Secondly, three methods are proposed to optimize the capacity of a CNN model on a task. The first method is based on improving the orthogonality among kernels which contributes to higher computational efficiency and better performance. In the second method, the convolutional kernels within each layer are evaluated according to their semantic functions and contributions to the training and test accuracy. The kernels which only contribute to the training accuracy but has no effect on the testing accuracy are removed to avoid overfitting. In the third method, the capacity of a CNN model is optimized by adjusting the dependency among convolutional kernels. A novel structure of convolutional layers is proposed to reduce the number of parameters while maintaining the similar performance. Besides capacity optimization, we further propose a method to evaluate the complexity of a human parsing task. An independent CNN model is trained for this purpose using the labels for pose estimation. The evaluation on complexity is achieved based on estimated pose information in images. The proposed scheme for complexity evaluation was conducted on the Pascal Person Part dataset and the Look into Person dataset which are for human parsing. The schemes for capacity optimization were conducted on our models for human parsing which were trained on the two data sets. Both quantitative and qualitative results demonstrate that our proposed algorithms can match the capacity of a CNN model well to the complexity of a task.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Capacity optimization,Complexity evaluation,Convolutional neural networks (CNNs),Over-fitting,Under-fitting},
  file = {C\:\\Users\\felix\\Zotero\\storage\\LJP9VL2V\\Jiang und Chi - 2020 - Constructing a Convolutional Neural Network with a.pdf}
}

@incollection{jindalDeepNeuralNetworks2020,
  title = {Deep {{Neural Networks}} for {{Corrupted Labels}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Jindal, Ishan and Nokleby, Matthew and Pressel, Daniel and Chen, Xuewen and Singh, Harpreet},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {211--235},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_7},
  abstract = {The success of deep convolutional networks on image and text classification and recognition tasks depends on the availability of large, correctly labeled training datasets, but obtaining the correct labels for these gigantic datasets is very difficult task. To deal with this problem, we describe an approach for learning deep networks from datasets corrupted by unknown label noise. We append a nonlinear noise model to a standard deep network, which is learned in tandem with the parameters of the network. Further, we train the network using a loss function that encourages the clustering of training images. We argue that the non-linear noise model, while not rigorous as a probabilistic model, results in a more effective denoising operator during backpropagation. We evaluate the performance of proposed approach on image classification task with artificially injected label noise to MNIST, CIFAR-10, CIFAR-100 and ImageNet datasets and on a large-scale Clothing 1M dataset with inherent label noise. Further, we show that with the different initialization and the regularization of the noise model, we can apply this learning procedure to text classification tasks as well. We evaluate the performance of modified approach on TREC text classification dataset. On all these datasets, the proposed approach provides significantly improved classification performance over the state of the art and is robust to the amount of label noise and the training samples. This approach is computationally fast, completely parallelizable, and easily implemented with existing machine learning libraries.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Convolutional network,Deep learning,E-M style label denoising,Image classification,Label noise,Text classification},
  file = {C\:\\Users\\felix\\Zotero\\storage\\9HEPYAUD\\Jindal et al. - 2020 - Deep Neural Networks for Corrupted Labels.pdf}
}

@inproceedings{karunakaranEfficientStatisticalValidation2020,
  title = {Efficient Statistical Validation with Edge Cases to Evaluate {{Highly Automated Vehicles}}},
  booktitle = {2020 {{IEEE}} 23rd {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Karunakaran, Dhanoop and Worrall, Stewart and Nebot, Eduardo},
  date = {2020-09},
  pages = {1--8},
  doi = {10.1109/ITSC45102.2020.9294590},
  abstract = {The widescale deployment of Autonomous Vehicles (AV) seems to be imminent despite many safety challenges that are yet to be resolved. It is well known that there are no universally agreed Verification and Validation (VV) methodologies to guarantee absolute safety, which is crucial for the acceptance of this technology. Existing standards focus on deterministic processes where the validation requires only a set of test cases that cover the requirements. Modern autonomous vehicles will undoubtedly include machine learning and probabilistic techniques that require a much more comprehensive testing regime due to the non-deterministic nature of the operating design domain. A rigourous statistical validation process is an essential component required to address this challenge. Most research in this area focuses on evaluating system performance in large scale real-world data gathering exercises (number of miles travelled), or randomised test scenarios in simulation. This paper presents a new approach to compute the statistical characteristics of a system's behaviour by biasing automatically generated test cases towards the worst case scenarios, identifying potential unsafe edge cases. We use reinforcement learning (RL) to learn the behaviours of simulated actors that cause unsafe behaviour measured by the well established RSS safety metric. We demonstrate that by using the method we can more efficiently validate a system using a smaller number of test cases by focusing the simulation towards the worst case scenario, generating edge cases that correspond to unsafe situations.},
  eventtitle = {2020 {{IEEE}} 23rd {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  keywords = {Accidents,Markov processes,Mathematical model,Reinforcement learning,Roads,Safety,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\KW4SCDNN\\Karunakaran et al. - 2020 - Efficient statistical validation with edge cases t.pdf;C\:\\Users\\felix\\Zotero\\storage\\I2AZF5UM\\9294590.html}
}

@software{KasguiKas2019,
  title = {Kas-Gui/Kas},
  origdate = {2019-10-25T10:58:42Z},
  url = {https://github.com/kas-gui/kas},
  urldate = {2021-12-13},
  abstract = {Another GUI toolkit},
  organization = {{kas-gui}}
}

@incollection{kaulTheoreticalCharacterizationDeep2020,
  title = {Theoretical {{Characterization}} of {{Deep Neural Networks}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Kaul, Piyush and Lall, Brejesh},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {25--63},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-31756-0_2},
  abstract = {Deep neural networks are poorly understood mathematically, however there has been a lot of recent work focusing on analyzing and understanding their success in a variety of pattern recognition tasks. We describe some of the mathematical techniques used for characterization of neural networks in terms of complexity of classification or regression task assigned, or based on functions learned, and try to relate this to architecture choices for neural networks. We explain some of the measurable quantifiers that can been used for defining expressivity of neural network including using homological complexity and curvature. We also describe neural networks from the viewpoints of scattering transforms and share some of the mathematical and intuitive justifications for those. We finally share a technique for visualizing and analyzing neural networks based on concept of Riemann curvature.},
  langid = {english},
  keywords = {Algebraic topology,Betti numbers,Curvature,Deep neural networks,Expressivity,Machine learning,Riemannian geometry,Scattering transform},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5EPR4XXY\\Kaul und Lall - 2020 - Theoretical Characterization of Deep Neural Networ.pdf}
}

@report{kiesEntwicklungUndAnalyse2020,
  title = {Entwicklung und Analyse von Auto-Encodern für intelligente Agenten zum Erlernen von Atari-Spielen},
  author = {Kies, Annika},
  date = {2020-09-11},
  langid = {german},
  file = {C\:\\Users\\felix\\Zotero\\storage\\3EY6THI2\\Kies - Entwicklung und Analyse von Auto-Encodern für inte.pdf}
}

@online{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2017-01-29},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\MYJ36B3F\\Kingma und Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\felix\\Zotero\\storage\\JLY5ACKM\\1412.html}
}

@online{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014-05-01},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\24DNFUVX\\Kingma und Welling - 2014 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\felix\\Zotero\\storage\\3W4WG8TJ\\1312.html}
}

@inproceedings{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classiﬁcation}} with {{Deep Convolutional Neural Networks}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  date = {2012},
  pages = {1097--1105},
  publisher = {{Advances in neural information processing systems}},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\6RVXJVZ3\\imagenet-classification-with-deep-convolutional-nn.pdf}
}

@incollection{kunteProgressNeuralNetwork2020,
  title = {Progress in {{Neural Network Based Statistical Language Modeling}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Kunte, Anup Shrikant and Attar, Vahida Z.},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {321--339},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_11},
  abstract = {Statistical Language Modeling (LM) is one of the central steps in many Natural Language Processing (NLP) tasks including Automatic Speech recognition (ASR), Statistical Machine Translation (SMT) , Sentence completion, Automatic Text Generation to name a few. Good Quality Language Model has been one of the key success factors for many commercial NLP applications. Since past three decades diverse research communities like psychology, neuroscience, data compression, machine translation, speech recognition, linguistics etc, have advanced research in the field of Language Modeling. First we understand the mathematical background of LM problem. Further we review various Neural Network based LM techniques in the order they were developed. We also review recent developments in Recurrent Neural Network (RNN) Based Language Models. Early LM research in ASR gave rise to commercially successful class of LMs called as N-gram LMs. These class of models were purely statistical based and lacked in utilising the linguistic information present in the text itself. With the advancement in the computing power, availability of humongous and rich sources of textual data Neural Network based LM paved their way into the arena. These techniques proved significant, since they mapped word tokens into a continuous space than treating them as discrete. As NNLM performance was proved to be comparable to existing state of the art N-gram LMs researchers also successfully used Deep Neural Network to LM. Researchers soon realised that the inherent sequential nature of textual input make LM problem a good Candidate for use of Recurrent Neural Network (RNN) architecture. Today RNN is the choice of Neural Architecture to solve LM by most practitioners. This chapter sheds light on variants of Neural Network Based LMs.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Artificial intelligence,Deep learning,Machine learning,Natural language processing,Statistical language modeling},
  file = {C\:\\Users\\felix\\Zotero\\storage\\QSGWQIDR\\Kunte und Attar - 2020 - Progress in Neural Network Based Statistical Langu.pdf}
}

@software{LinebenderDruid2018,
  title = {Linebender/Druid},
  origdate = {2018-11-01T22:25:17Z},
  url = {https://github.com/linebender/druid},
  urldate = {2021-12-13},
  abstract = {A data-first Rust-native UI design toolkit.},
  organization = {{linebender}}
}

@report{loranPraktikumsberichtEntwicklungAutoencodern,
  title = {Praktikumsbericht: Entwicklung von Autoencodern für intelligente Agenten basierend auf neuartigen neuronalen Netzen},
  author = {Loran, Dennis},
  pages = {13},
  langid = {german},
  file = {C\:\\Users\\felix\\Zotero\\storage\\UJYQLRJS\\Loran - Praktikumsbericht Entwicklung von Autoencodern fü.pdf}
}

@article{luanAromaCodeRecommendation2019,
  title = {Aroma: Code Recommendation via Structural Code Search},
  shorttitle = {Aroma},
  author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
  date = {2019-10-10},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  shortjournal = {Proc. ACM Program. Lang.},
  volume = {3},
  pages = {152:1--152:28},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3360578},
  abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.},
  issue = {OOPSLA},
  keywords = {clone detection,clustering,code recommendation,feature-based code representation,structural code search},
  file = {C\:\\Users\\felix\\Zotero\\storage\\3BQKSASA\\Luan et al. - 2019 - Aroma code recommendation via structural code sea.pdf}
}

@article{luanAromaCodeRecommendation2019a,
  title = {Aroma: {{Code}} Recommendation via Structural Code Search},
  author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
  date = {2019},
  journaltitle = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  pages = {1--28},
  publisher = {{ACM New York, NY, USA}},
  issue = {OOPSLA}
}

@article{luanAromaCodeRecommendation2019b,
  title = {Aroma: {{Code Recommendation}} via {{Structural Code Search}}},
  author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
  date = {2019-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {3},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3360578},
  abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.},
  issue = {OOPSLA},
  keywords = {clone detection,clustering,code recommendation,feature-based code representation,structural code search}
}

@article{luanAromaCodeRecommendation2019c,
  title = {Aroma: Code Recommendation via Structural Code Search},
  author = {Luan, Sifei and Yang, Di and Barnaby, Celeste and Sen, Koushik and Chandra, Satish},
  date = {2019-10},
  journaltitle = {Proc. ACM Program. Lang.},
  volume = {3},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3360578},
  abstract = {Programmers often write code that has similarity to existing code written somewhere. A tool that could help programmers to search such similar code would be immensely useful. Such a tool could help programmers to extend partially written code snippets to completely implement necessary functionality, help to discover extensions to the partial code which are commonly included by other programmers, help to cross-check against similar code written by other programmers, or help to add extra code which would fix common mistakes and errors. We propose Aroma, a tool and technique for code recommendation via structural code search. Aroma indexes a huge code corpus including thousands of open-source projects, takes a partial code snippet as input, searches the corpus for method bodies containing the partial code snippet, and clusters and intersects the results of the search to recommend a small set of succinct code snippets which both contain the query snippet and appear as part of several methods in the corpus. We evaluated Aroma on 2000 randomly selected queries created from the corpus, as well as 64 queries derived from code snippets obtained from Stack Overflow, a popular website for discussing code. We implemented Aroma for 4 different languages, and developed an IDE plugin for Aroma. Furthermore, we conducted a study where we asked 12 programmers to complete programming tasks using Aroma, and collected their feedback. Our results indicate that Aroma is capable of retrieving and recommending relevant code snippets efficiently.},
  issue = {OOPSLA},
  keywords = {clone detection,clustering,code recommendation,feature-based code representation,structural code search}
}

@incollection{malitaHeterogeneousComputingSystem2020,
  title = {Heterogeneous {{Computing System}} for {{Deep Learning}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Maliţa, Mihaela and Popescu, George Vlǎduţ and Ştefan, Gheorghe M.},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {287--319},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_10},
  abstract = {Various forms of Deep Neural Network (DNN) architectures are used as Deep Learning tools for neural inspired computational systems. The computational power, the bandwidth and the energy requested by the current developments of the domain are very high. The solutions offered by the current architectural environment are far from being efficient. We propose a hybrid computational system for running efficiently the training and inference DNN algorithms. The system is more energy efficient compared with the current solutions, and achieves a higher actual performance per peak performance ratio. The accelerator part of our heterogeneous system is a programmable many-core system with a Map-Scan/Reductive only the cells where architecture. The chapter describes and evaluates the proposed accelerator for the main computational intensive components of a DNN: the fully connected layer, the convolution layer, the pooling layer, and the softmax layer.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Accelerators,Deep neural network,Heterogeneous computing,Parallel computing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\VK7QYDL8\\Maliţa et al. - 2020 - Heterogeneous Computing System for Deep Learning.pdf}
}

@online{manakovWalkingTightropeInvestigation2020,
  title = {Walking the {{Tightrope}}: {{An Investigation}} of the {{Convolutional Autoencoder Bottleneck}}},
  shorttitle = {Walking the {{Tightrope}}},
  author = {Manakov, Ilja and Rohm, Markus and Tresp, Volker},
  date = {2020-05-12},
  eprint = {1911.07460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07460},
  urldate = {2021-11-25},
  abstract = {In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization in terms of reconstruction error while also speeding up training. The number of channels in the bottleneck, on the other hand, is of secondary importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when all layers have the same number of neurons as there are pixels in the input. Copying does not occur, even when training the CAE for 1,000 epochs on a tiny (≈ 600 images) dataset. Besides raising important questions for further research, our findings are directly applicable to two of the most common use-cases for CAEs: In image compression, it is advantageous to increase the feature map size in the bottleneck as this improves reconstruction quality greatly. For reconstruction-based outlier detection, we recommend decreasing the feature map size so that out-of-distribution samples will yield a higher reconstruction error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5KW84BJU\\Manakov et al. - 2020 - Walking the Tightrope An Investigation of the Con.pdf}
}

@online{manakovWalkingTightropeInvestigation2020a,
  title = {Walking the {{Tightrope}}: {{An Investigation}} of the {{Convolutional Autoencoder Bottleneck}}},
  shorttitle = {Walking the {{Tightrope}}},
  author = {Manakov, Ilja and Rohm, Markus and Tresp, Volker},
  date = {2020-05-12},
  eprint = {1911.07460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07460},
  urldate = {2021-11-25},
  abstract = {In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization in terms of reconstruction error while also speeding up training. The number of channels in the bottleneck, on the other hand, is of secondary importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when all layers have the same number of neurons as there are pixels in the input. Copying does not occur, even when training the CAE for 1,000 epochs on a tiny (≈ 600 images) dataset. Besides raising important questions for further research, our findings are directly applicable to two of the most common use-cases for CAEs: In image compression, it is advantageous to increase the feature map size in the bottleneck as this improves reconstruction quality greatly. For reconstruction-based outlier detection, we recommend decreasing the feature map size so that out-of-distribution samples will yield a higher reconstruction error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\92HJ5EM4\\Manakov et al. - 2020 - Walking the Tightrope An Investigation of the Con.pdf}
}

@online{manakovWalkingTightropeInvestigation2020b,
  title = {Walking the {{Tightrope}}: {{An Investigation}} of the {{Convolutional Autoencoder Bottleneck}}},
  shorttitle = {Walking the {{Tightrope}}},
  author = {Manakov, Ilja and Rohm, Markus and Tresp, Volker},
  date = {2020-05-12},
  eprint = {1911.07460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07460},
  urldate = {2021-11-25},
  abstract = {In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization in terms of reconstruction error while also speeding up training. The number of channels in the bottleneck, on the other hand, is of secondary importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when all layers have the same number of neurons as there are pixels in the input. Copying does not occur, even when training the CAE for 1,000 epochs on a tiny (≈ 600 images) dataset. Besides raising important questions for further research, our findings are directly applicable to two of the most common use-cases for CAEs: In image compression, it is advantageous to increase the feature map size in the bottleneck as this improves reconstruction quality greatly. For reconstruction-based outlier detection, we recommend decreasing the feature map size so that out-of-distribution samples will yield a higher reconstruction error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\R83AZ5TY\\Manakov et al. - 2020 - Walking the Tightrope An Investigation of the Con.pdf}
}

@online{manakovWalkingTightropeInvestigation2020c,
  title = {Walking the {{Tightrope}}: {{An Investigation}} of the {{Convolutional Autoencoder Bottleneck}}},
  shorttitle = {Walking the {{Tightrope}}},
  author = {Manakov, Ilja and Rohm, Markus and Tresp, Volker},
  date = {2020-05-12},
  eprint = {1911.07460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07460},
  urldate = {2021-11-25},
  abstract = {In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization in terms of reconstruction error while also speeding up training. The number of channels in the bottleneck, on the other hand, is of secondary importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when all layers have the same number of neurons as there are pixels in the input. Copying does not occur, even when training the CAE for 1,000 epochs on a tiny (≈ 600 images) dataset. Besides raising important questions for further research, our findings are directly applicable to two of the most common use-cases for CAEs: In image compression, it is advantageous to increase the feature map size in the bottleneck as this improves reconstruction quality greatly. For reconstruction-based outlier detection, we recommend decreasing the feature map size so that out-of-distribution samples will yield a higher reconstruction error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DNCSUCUW\\Manakov et al. - 2020 - Walking the Tightrope An Investigation of the Con.pdf}
}

@online{manakovWalkingTightropeInvestigation2020d,
  title = {Walking the {{Tightrope}}: {{An Investigation}} of the {{Convolutional Autoencoder Bottleneck}}},
  shorttitle = {Walking the {{Tightrope}}},
  author = {Manakov, Ilja and Rohm, Markus and Tresp, Volker},
  date = {2020-05-12},
  eprint = {1911.07460},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1911.07460},
  urldate = {2021-11-25},
  abstract = {In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from outlier detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks. The number of channels in the bottleneck, on the other hand, is secondary in importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Copying does not occur, despite training the CAE for 1,000 epochs on a tiny (\$\textbackslash approx\$ 600 images) dataset. We believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SWAHHF7N\\Manakov et al. - 2020 - Walking the Tightrope An Investigation of the Con.pdf;C\:\\Users\\felix\\Zotero\\storage\\TU5HVPMF\\1911.html}
}

@article{mandelinJungloidMiningHelping2005,
  title = {Jungloid Mining: Helping to Navigate the {{API}} Jungle},
  shorttitle = {Jungloid Mining},
  author = {Mandelin, David and Xu, Lin and Bodík, Rastislav and Kimelman, Doug},
  date = {2005-06-12},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  volume = {40},
  number = {6},
  pages = {48--61},
  issn = {0362-1340},
  doi = {10.1145/1064978.1065018},
  abstract = {Reuse of existing code from class libraries and frameworks is often difficult because APIs are complex and the client code required to use the APIs can be hard to write. We observed that a common scenario is that the programmer knows what type of object he needs, but does not know how to write the code to get the object.In order to help programmers write API client code more easily, we developed techniques for synthesizing jungloid code fragments automatically given a simple query that describes that desired code in terms of input and output types. A jungloid is simply a unary expression; jungloids are simple, enabling synthesis, but are also versatile, covering many coding problems, and composable, combining to form more complex code fragments. We synthesize jungloids using both API method signatures and jungloids mined from a corpus of sample client programs.We implemented a tool, prospector, based on these techniques. prospector is integrated with the Eclipse IDE code assistance feature, and it infers queries from context so there is no need for the programmer to write queries. We tested prospector on a set of real programming problems involving APIs; prospector found the desired solution for 18 of 20 problems. We also evaluated prospector in a user study, finding that programmers solved programming problems more quickly and with more reuse when using prospector than without prospector.},
  keywords = {mining,program synthesis,reuse},
  file = {C\:\\Users\\felix\\Zotero\\storage\\QVK5YV68\\Mandelin et al. - 2005 - Jungloid mining helping to navigate the API jungl.pdf}
}

@inproceedings{marianiAutoBlackTestAutomaticBlackBox2012,
  title = {{{AutoBlackTest}}: {{Automatic Black-Box Testing}} of {{Interactive Applications}}},
  shorttitle = {{{AutoBlackTest}}},
  booktitle = {Verification and {{Validation}} 2012 {{IEEE Fifth International Conference}} on {{Software Testing}}},
  author = {Mariani, Leonardo and Pezze, Mauro and Riganelli, Oliviero and Santoro, Mauro},
  date = {2012-04},
  pages = {81--90},
  issn = {2159-4848},
  doi = {10.1109/ICST.2012.88},
  abstract = {Automatic test case generation is a key ingredient of an efficient and cost-effective software verification process. In this paper we focus on testing applications that interact with the users through a GUI, and present AutoBlackTest, a technique to automatically generate test cases at the system level. AutoBlackTest uses reinforcement learning, in particular Q-Learning, to learn how to interact with the application under test and stimulate its functionalities. The empirical results show that AutoBlackTest can execute a relevant portion of the code of the application under test, and can reveal previously unknown problems by working at the system level and interacting only through the GUI.},
  eventtitle = {Verification and {{Validation}} 2012 {{IEEE Fifth International Conference}} on {{Software Testing}}},
  keywords = {Analytical models,Black-Box Testing,Concrete,Databases,Graphical user interfaces,Observers,Prototypes,Q-Learning,Test Automation,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SFTSJ3C8\\Mariani et al. - 2012 - AutoBlackTest Automatic Black-Box Testing of Inte.pdf;C\:\\Users\\felix\\Zotero\\storage\\VUEUWS6W\\6200099.html}
}

@inproceedings{marianiAutoBlackTestAutomaticBlackBox2012a,
  title = {{{AutoBlackTest}}: {{Automatic Black-Box Testing}} of {{Interactive Applications}}},
  shorttitle = {{{AutoBlackTest}}},
  booktitle = {Verification and {{Validation}} 2012 {{IEEE Fifth International Conference}} on {{Software Testing}}},
  author = {Mariani, Leonardo and Pezze, Mauro and Riganelli, Oliviero and Santoro, Mauro},
  date = {2012-04},
  pages = {81--90},
  issn = {2159-4848},
  doi = {10.1109/ICST.2012.88},
  abstract = {Automatic test case generation is a key ingredient of an efficient and cost-effective software verification process. In this paper we focus on testing applications that interact with the users through a GUI, and present AutoBlackTest, a technique to automatically generate test cases at the system level. AutoBlackTest uses reinforcement learning, in particular Q-Learning, to learn how to interact with the application under test and stimulate its functionalities. The empirical results show that AutoBlackTest can execute a relevant portion of the code of the application under test, and can reveal previously unknown problems by working at the system level and interacting only through the GUI.},
  eventtitle = {Verification and {{Validation}} 2012 {{IEEE Fifth International Conference}} on {{Software Testing}}},
  keywords = {Analytical models,Black-Box Testing,Concrete,Databases,Graphical user interfaces,Observers,Prototypes,Q-Learning,Test Automation,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\A3BVC8UR\\6200099.html}
}

@inproceedings{mensSourceCodebasedRecommendation2014,
  title = {Source Code-Based Recommendation Systems},
  booktitle = {Recommendation {{Systems}} in {{Software Engineering}}},
  author = {Mens, Kim and Lozano, Angela},
  editor = {Robillard, Martin P. and Maalej, Walid and Walker, Robert J. and Zimmermann, Thomas},
  date = {2014},
  pages = {93--130},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-45135-5_5},
  abstract = {Although today’s software systems are composed of a diversity of software artifacts, source code remains the most up-to-date artifact and therefore the most reliable data source. It provides a rich and structured source of information upon which recommendation systems can rely to provide useful recommendations to software developers. Source code-based recommendation systems provide support for tasks such as how to use a given API or framework, provide hints on things missing from the code, suggest how to reuse or correct an existing code, or help novices learn a new project, programming paradigm, language, or style. This chapter highlights relevant decisions involved in developing source code-based recommendation systems. An in-depth presentation of a particular system we developed serves as a concrete illustration of some of the issues that can be encountered and of the development choices that need to be made when building such a system.},
  isbn = {978-3-642-45135-5},
  langid = {english},
  keywords = {Application Programming Interface,Association Rule,Association Rule Mining,Recommendation System,Source Code},
  annotation = {10.1007/978-3-642-45135-5\_5},
  file = {C\:\\Users\\felix\\Zotero\\storage\\A6EKHT76\\Mens und Lozano - 2014 - Source Code-Based Recommendation Systems.pdf}
}

@inproceedings{muratetAccessibilitySeriousGames2020,
  title = {Accessibility and {{Serious Games}}: {{What About Entity-Component-System Software Architecture}}?},
  shorttitle = {Accessibility and {{Serious Games}}},
  booktitle = {Games and {{Learning Alliance}}},
  author = {Muratet, Mathieu and Garbarini, Délia},
  editor = {Marfisi-Schottman, Iza and Bellotti, Francesco and Hamon, Ludovic and Klemke, Roland},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {3--12},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-63464-3_1},
  abstract = {Video games are an integral part of popular culture. The video game industry faces challenges with the increase in players’ numbers and application areas including serious games. The increase in the number of players includes disabled players. Therefore, serious games have to consider these audiences who may be affected by one or more temporary or ongoing disabilities. Universally accessible games (UA-Games) aim to create interfaces that can be accessed and manipulated by the largest number of players. Currently few serious games include accessibility features, while accessibility should be considered at the beginning of the serious game design. Then how can we help designers and developers to include accessible features in an existing serious game? To this end, game engines must be efficient but also scalable and modular. This paper studies the interest of the Entity-Component-System (ECS) software architecture to integrate accessibility features in an existing serious game. As a case study, we will take a serious game developed with ECS yet not accessible: E-LearningScape. We will present the accessible features that we have integrated into the serious game and discuss the pros and cons of this approach. This feedback shows us that ECS provides very useful design flexibility to integrate unanticipated interaction features into serious games.},
  langid = {english},
  keywords = {Accessibility feature,Data oriented programming,Entity-Component-System,Serious games,Software architecture,Universal design},
  file = {C\:\\Users\\felix\\Zotero\\storage\\K2RTMKX6\\Muratet und Garbarini - 2020 - Accessibility and Serious Games What About Entity.pdf}
}

@article{murshedMachineLearningNetwork2019,
  title = {Machine {{Learning}} at the {{Network Edge}}: {{A Survey}}},
  shorttitle = {Machine {{Learning}} at the {{Network Edge}}},
  author = {Murshed, M. G. Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz},
  date = {2019-07-31},
  doi = {10.1145/3469029},
  abstract = {Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\7AXHTYS3\\Murshed et al. - 2019 - Machine Learning at the Network Edge A Survey.pdf;C\:\\Users\\felix\\Zotero\\storage\\52H9PIDK\\1908.html}
}

@inproceedings{mussbacherAssessmentGridIntelligent2020,
  title = {Towards an Assessment Grid for Intelligent Modeling Assistance},
  booktitle = {Proceedings of the 23rd {{ACM}}/{{IEEE International Conference}} on {{Model Driven Engineering Languages}} and {{Systems}}: {{Companion Proceedings}}},
  author = {Mussbacher, Gunter and Combemale, Benoit and Abrahão, Silvia and Bencomo, Nelly and Burgueño, Loli and Engels, Gregor and Kienzle, Jörg and Kühn, Thomas and Mosser, Sébastien and Sahraoui, Houari and Weyssow, Martin},
  date = {2020-10-16},
  series = {{{MODELS}} '20},
  pages = {1--10},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3417990.3421396},
  abstract = {The ever-growing complexity of systems, the growing number of stakeholders, and the corresponding continuous emergence of new domain-specific modeling abstractions has led to significantly higher cognitive load on modelers. There is an urgent need to provide modelers with better, more Intelligent Modeling Assistants (IMAs). An important factor to consider is the ability to assess and compare, to learn from existing and inform future IMAs, while potentially combining them. Recently, a conceptual Reference Framework for Intelligent Modeling Assistance (RF-IMA) was proposed. RF-IMA defines the main required components and high-level properties of IMAs. In this paper, we present a detailed, level-wise definition for the properties of RF-IMA to enable a better understanding, comparison, and selection of existing and future IMAs. The proposed levels are a first step towards a comprehensive assessment grid for intelligent modeling assistance. For an initial validation of the proposed levels, we assess the existing landscape of intelligent modeling assistance and three future scenarios of intelligent modeling assistance against these levels.},
  isbn = {978-1-4503-8135-2},
  keywords = {artificial intelligence,assessment levels,feedback,integrated development environment,intelligent modeling assistance,model-based software engineering},
  file = {C\:\\Users\\felix\\Zotero\\storage\\32H3YVDN\\Mussbacher et al. - 2020 - Towards an assessment grid for intelligent modelin.pdf}
}

@book{myersArtSoftwareTesting2011,
  title = {The {{Art}} of {{Software Testing}}},
  author = {Myers, Glenford J. and Sandler, Corey and Badgett, Tom},
  date = {2011},
  publisher = {{John Wiley \& Sons, Incorporated}},
  location = {{Hoboken, UNITED STATES}},
  isbn = {978-1-118-13313-2},
  keywords = {Computer software -- Testing.,Debugging in computer science.},
  file = {C\:\\Users\\felix\\Zotero\\storage\\EU4IWQ9W\\reader.html}
}

@online{NeuroevolutionaiMonkeyTestingDummyApp,
  title = {Neuroevolution-Ai/{{MonkeyTestingDummyApp}}},
  url = {https://github.com/neuroevolution-ai/MonkeyTestingDummyApp},
  urldate = {2021-05-05},
  abstract = {Contribute to neuroevolution-ai/MonkeyTestingDummyApp development by creating an account on GitHub.},
  langid = {english},
  organization = {{GitHub}},
  file = {C\:\\Users\\felix\\Zotero\\storage\\SH2Y7MGQ\\Jadx Environment Documentation.html}
}

@inproceedings{nguyenAPICodeRecommendation2016,
  title = {{{API}} Code Recommendation Using Statistical Learning from Fine-Grained Changes},
  booktitle = {Proceedings of the 2016 24th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of {{Software Engineering}}},
  author = {Nguyen, Anh Tuan and Hilton, Michael and Codoban, Mihai and Nguyen, Hoan Anh and Mast, Lily and Rademacher, Eli and Nguyen, Tien N. and Dig, Danny},
  date = {2016-11-01},
  series = {{{FSE}} 2016},
  pages = {511--522},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2950290.2950333},
  abstract = {Learning and remembering how to use APIs is difficult. While code-completion tools can recommend API methods, browsing a long list of API method names and their documentation is tedious. Moreover, users can easily be overwhelmed with too much information. We present a novel API recommendation approach that taps into the predictive power of repetitive code changes to provide relevant API recommendations for developers. Our approach and tool, APIREC, is based on statistical learning from fine-grained code changes and from the context in which those changes were made. Our empirical evaluation shows that APIREC correctly recommends an API call in the first position 59\% of the time, and it recommends the correct API call in the top five positions 77\% of the time. This is a significant improvement over the state-of-the-art approaches by 30-160\% for top-1 accuracy, and 10-30\% for top-5 accuracy, respectively. Our result shows that APIREC performs well even with a one-time, minimal training dataset of 50 publicly available projects.},
  isbn = {978-1-4503-4218-6},
  keywords = {API Recommendation,Fine-grained Code Changes,Statistical Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\JJS8ZPC7\\Nguyen et al. - 2016 - API code recommendation using statistical learning.pdf;C\:\\Users\\felix\\Zotero\\storage\\LWSTG229\\Nguyen et al. - 2016 - API code recommendation using statistical learning.pdf}
}

@inproceedings{nguyenGraPaccGraphbasedPatternoriented2012,
  title = {{{GraPacc}}: A Graph-Based Pattern-Oriented, Context-Sensitive Code Completion Tool},
  shorttitle = {Grapacc},
  booktitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Nguyen, A. T. and Nguyen, H. A. and Nguyen, T. T. and Nguyen, T. N.},
  date = {2012-06},
  pages = {1407--1410},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2012.6227236},
  abstract = {Code completion tool plays an important role in daily development activities. It helps developers by auto-completing tedious and detailed code during an editing session. However, existing code completion tools are limited to recommending only context-free code templates and a single method call of the variable under editing. We introduce GraPacc, an advanced, context-sensitive code completion tool that is based on frequent API usage patterns. It extracts the context-sensitive features from the code under editing, for example, the API elements on focus and the current editing point, and their relations to other code elements. It then ranks the relevant API usage patterns and auto-completes the current code with the proper elements according to the chosen pattern.},
  eventtitle = {2012 34th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {API usage pattern,application program interfaces,Context,context-free code templates,context-sensitive code completion tool,editing session,Feature extraction,GraPacc,graph-based pattern-oriented code completion tool,Indexes,Pattern-oriented Code Completion,Programming,software tools,Switches,Usage Pattern,Vectors},
  file = {C\:\\Users\\felix\\Zotero\\storage\\23X7R39H\\Nguyen et al. - 2012 - GraPacc A graph-based pattern-oriented, context-s.pdf;C\:\\Users\\felix\\Zotero\\storage\\8ATCALH5\\6227236.html}
}

@article{nguyenGUITARInnovativeTool2014,
  title = {{{GUITAR}}: An Innovative Tool for Automated Testing of {{GUI-driven}} Software},
  shorttitle = {{{GUITAR}}},
  author = {Nguyen, Bao N. and Robbins, Bryan and Banerjee, Ishan and Memon, Atif},
  date = {2014-03-01},
  journaltitle = {Automated Software Engineering},
  shortjournal = {Autom Softw Eng},
  volume = {21},
  number = {1},
  pages = {65--105},
  doi = {10.1007/s10515-013-0128-9},
  abstract = {Most of today’s software applications feature a graphical user interface (GUI) front-end. System testing of these applications requires that test cases, modeled as sequences of GUI events, be generated and executed on the software. We term GUI testing as the process of testing a software application through its GUI. Researchers and practitioners agree that one must employ a variety of techniques (e.g., model-based, capture/replay, manually scripted) for effective GUI testing. Yet, the tools available today for GUI testing are limited in the techniques they support. In this paper, we describe an innovative tool called GUITAR that supports a wide variety of GUI testing techniques. The innovation lies in the architecture of GUITAR, which uses plug-ins to support flexibility and extensibility. Software developers and quality assurance engineers may use this architecture to create new toolchains, new workflows based on the toolchains, and plug in a variety of measurement tools to conduct GUI testing. We demonstrate these features of GUITAR via several carefully crafted case studies.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\QNDXRB3X\\Nguyen et al. - 2014 - GUITAR an innovative tool for automated testing o.pdf}
}

@article{nyamaweRecommendingRefactoringSolutions2018,
  title = {Recommending Refactoring Solutions Based on Traceability and Code Metrics},
  author = {Nyamawe, A. S. and Liu, H. and Niu, Z. and Wang, W. and Niu, N.},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {49460--49475},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2868990},
  abstract = {Software refactoring has been extensively used to rectify the design flaws and improve software quality without affecting its observable behaviors. For a given code smell, it is common that there exist multiple refactoring solutions. However, it is challenging for developers to select the best one from such potential solutions. Consequently, a number of approaches have been proposed to facilitate the selection. Such approaches compare and select among alternative refactoring solutions based on their impact on metrics of source code. However, their impact on the traceability between source code and requirements is ignored although the importance of such traceability has been well recognized. To this end, we select among alternative refactoring solutions according to how they improve the traceability as well as source code design. To quantify the quality of traceability and source code design we leverage the use of entropybased and traditional coupling and cohesion metrics respectively. We virtually apply alternative refactoring solutions and measure their effect on the traceability and source code design. The one leading to greatest improvement is recommended. The proposed approach has been evaluated on a well-known data set. The evaluation results suggest that on up to 71\% of the cases, developers prefer our recommendation to the traditional recommendation based on code metrics.},
  eventtitle = {{{IEEE Access}}},
  keywords = {alternative refactoring solutions,code metrics,code smell,Couplings,Entropy,Measurement,multiple refactoring solutions,program diagnostics,refactoring,refactoring Solution recommendation,requirements traceability,software maintenance,Software maintenance,software metrics,software quality,software refactoring,Software systems,solution recommendation,source code (software),source code design,Tools,traceability},
  file = {C\:\\Users\\felix\\Zotero\\storage\\Y22Y5HVI\\Nyamawe et al. - 2018 - Recommending Refactoring Solutions Based on Tracea.pdf;C\:\\Users\\felix\\Zotero\\storage\\WTL7MX8K\\8456513.html}
}

@article{nyamaweRecommendingRefactoringSolutions2018a,
  title = {Recommending Refactoring Solutions Based on Traceability and Code Metrics},
  author = {Nyamawe, A. S. and Liu, H. and Niu, Z. and Wang, W. and Niu, N.},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {49460--49475},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2868990},
  abstract = {Software refactoring has been extensively used to rectify the design flaws and improve software quality without affecting its observable behaviors. For a given code smell, it is common that there exist multiple refactoring solutions. However, it is challenging for developers to select the best one from such potential solutions. Consequently, a number of approaches have been proposed to facilitate the selection. Such approaches compare and select among alternative refactoring solutions based on their impact on metrics of source code. However, their impact on the traceability between source code and requirements is ignored although the importance of such traceability has been well recognized. To this end, we select among alternative refactoring solutions according to how they improve the traceability as well as source code design. To quantify the quality of traceability and source code design we leverage the use of entropybased and traditional coupling and cohesion metrics respectively. We virtually apply alternative refactoring solutions and measure their effect on the traceability and source code design. The one leading to greatest improvement is recommended. The proposed approach has been evaluated on a well-known data set. The evaluation results suggest that on up to 71\% of the cases, developers prefer our recommendation to the traditional recommendation based on code metrics.},
  eventtitle = {{{IEEE Access}}},
  keywords = {alternative refactoring solutions,code metrics,code smell,Couplings,Entropy,Measurement,multiple refactoring solutions,program diagnostics,refactoring,refactoring Solution recommendation,requirements traceability,software maintenance,Software maintenance,software metrics,software quality,software refactoring,Software systems,solution recommendation,source code (software),source code design,Tools,traceability},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5633ZMEH\\Nyamawe et al. - 2018 - Recommending Refactoring Solutions Based on Tracea.pdf;C\:\\Users\\felix\\Zotero\\storage\\XL3WEPIK\\8456513.html}
}

@article{nyamaweRecommendingRefactoringSolutions2018b,
  title = {Recommending Refactoring Solutions Based on Traceability and Code Metrics},
  author = {Nyamawe, A. S. and Liu, H. and Niu, Z. and Wang, W. and Niu, N.},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {49460--49475},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2868990},
  abstract = {Software refactoring has been extensively used to rectify the design flaws and improve software quality without affecting its observable behaviors. For a given code smell, it is common that there exist multiple refactoring solutions. However, it is challenging for developers to select the best one from such potential solutions. Consequently, a number of approaches have been proposed to facilitate the selection. Such approaches compare and select among alternative refactoring solutions based on their impact on metrics of source code. However, their impact on the traceability between source code and requirements is ignored although the importance of such traceability has been well recognized. To this end, we select among alternative refactoring solutions according to how they improve the traceability as well as source code design. To quantify the quality of traceability and source code design we leverage the use of entropybased and traditional coupling and cohesion metrics respectively. We virtually apply alternative refactoring solutions and measure their effect on the traceability and source code design. The one leading to greatest improvement is recommended. The proposed approach has been evaluated on a well-known data set. The evaluation results suggest that on up to 71\% of the cases, developers prefer our recommendation to the traditional recommendation based on code metrics.},
  eventtitle = {{{IEEE Access}}},
  keywords = {alternative refactoring solutions,code metrics,code smell,Couplings,Entropy,Measurement,multiple refactoring solutions,program diagnostics,refactoring,refactoring Solution recommendation,requirements traceability,software maintenance,Software maintenance,software metrics,software quality,software refactoring,Software systems,solution recommendation,source code (software),source code design,Tools,traceability},
  file = {C\:\\Users\\felix\\Zotero\\storage\\C6J5358L\\Nyamawe et al. - 2018 - Recommending Refactoring Solutions Based on Tracea.pdf}
}

@article{nymanUsingMonkeyTest2000,
  title = {Using Monkey Test Tools},
  author = {Nyman, Noel},
  date = {2000},
  journaltitle = {Soft. Testing and Quality Eng.},
  file = {C\:\\Users\\felix\\Zotero\\storage\\TSXR95AP\\Smzr1XDD2604filelistfilename1_0.pdf}
}

@article{ouniMaintainabilityDefectsDetection2013,
  title = {Maintainability Defects Detection and Correction: A Multi-Objective Approach},
  shorttitle = {Maintainability Defects Detection and Correction},
  author = {Ouni, Ali and Kessentini, Marouane and Sahraoui, Houari and Boukadoum, Mounir},
  date = {2013-03-01},
  journaltitle = {Automated Software Engineering},
  shortjournal = {Autom Softw Eng},
  volume = {20},
  number = {1},
  pages = {47--79},
  issn = {1573-7535},
  doi = {10.1007/s10515-011-0098-8},
  abstract = {Software defects often lead to bugs, runtime errors and software maintenance difficulties. They should be systematically prevented, found, removed or fixed all along the software lifecycle. However, detecting and fixing these defects is still, to some extent, a difficult, time-consuming and manual process. In this paper, we propose a two-step automated approach to detect and then to correct various types of maintainability defects in source code. Using Genetic Programming, our approach allows automatic generation of rules to detect defects, thus relieving the designer from a fastidious manual rule definition task. Then, we correct the detected defects while minimizing the correction effort. A correction solution is defined as the combination of refactoring operations that should maximize as much as possible the number of corrected defects with minimal code modification effort. We use the Non-dominated Sorting Genetic Algorithm (NSGA-II) to find the best compromise. For six open source projects, we succeeded in detecting the majority of known defects, and the proposed corrections fixed most of them with minimal effort.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\RPQBRJLV\\Ouni et al. - 2013 - Maintainability defects detection and correction .pdf}
}

@article{ouniMOREMultiobjectiveRefactoring2017,
  title = {{{MORE}}: A Multi-Objective Refactoring Recommendation Approach to Introducing Design Patterns and Fixing Code Smells},
  shorttitle = {{{MORE}}},
  author = {Ouni, Ali and Kessentini, Marouane and Ó Cinnéide, Mel and Sahraoui, Houari and Deb, Kalyanmoy and Inoue, Katsuro},
  date = {2017},
  journaltitle = {Journal of Software: Evolution and Process},
  volume = {29},
  number = {5},
  pages = {e1843},
  issn = {2047-7481},
  doi = {10.1002/smr.1843},
  abstract = {Refactoring is widely recognized as a crucial technique applied when evolving object-oriented software systems. If applied well, refactoring can improve different aspects of software quality including readability, maintainability, and extendibility. However, despite its importance and benefits, recent studies report that automated refactoring tools are underused much of the time by software developers. This paper introduces an automated approach for refactoring recommendation, called MORE, driven by 3 objectives: (1) to improve design quality (as defined by software quality metrics), (2) to fix code smells, and (3) to introduce design patterns. To this end, we adopt the recent nondominated sorting genetic algorithm, NSGA-III, to find the best trade-off between these 3 objectives. We evaluated the efficacy of our approach using a benchmark of 7 medium and large open-source systems, 7 commonly occurring code smells (god class, feature envy, data class, spaghetti code, shotgun surgery, lazy class, and long parameter list), and 4 common design pattern types (visitor, factory method, singleton, and strategy). Our approach is empirically evaluated through a quantitative and qualitative study to compare it against 3 different state-of-the art approaches, 2 popular multiobjective search algorithms, and random search. The statistical analysis of the results confirms the efficacy of our approach in improving the quality of the studied systems while successfully fixing 84\% of code smells and introducing an average of 6 design patterns. In addition, the qualitative evaluation shows that most of the suggested refactorings (an average of 69\%) are considered by developers to be relevant and meaningful.},
  langid = {english},
  keywords = {code smells,design patterns,refactoring,search-based software engineering,software quality},
  file = {C\:\\Users\\felix\\Zotero\\storage\\2B44DHRL\\Ouni et al. - 2017 - MORE A multi-objective refactoring recommendation.pdf;C\:\\Users\\felix\\Zotero\\storage\\X5HEDTAC\\smr.html}
}

@incollection{pawarAssessmentAutoencoderArchitectures2020,
  title = {Assessment of {{Autoencoder Architectures}} for {{Data Representation}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Pawar, Karishma and Attar, Vahida Z.},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {101--132},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_4},
  abstract = {Efficient representation learning of data distribution is part and parcel of successful execution of any machine learning based model. Autoencoders are good at learning the representation of data with lower dimensions. Traditionally, autoencoders have been widely used for data compression in order to represent the structural data. Data compression is one of the most important tasks in applications based on Computer Vision, Information Retrieval, Natural Language Processing, etc. The aim of data compression is to convert the input data into smaller representation retaining the quality of input data. Many lossy and lossless data compression techniques like Flate/deflate compression, Lempel–Ziv–Welch compression, Huffman compression, Run-length encoding compression, JPEG compression are available. Similarly, autoencoders are unsupervised neural networks used for representing the structural data by data compression. Due to wide availability of high-end processing chips and large datasets, deep learning has gained a lot attention from academia, industries and research centers to solve multitude of problems. Considering the state-of-the-art literature, autoencoders are widely used architectures in many deep learning applications for representation and manifold learning and serve as popular option for dimensionality reduction. Therefore, this chapter aims to shed light upon applicability of variants of autoencoders to multiple application domains. In this chapter, basic architecture and variants of autoencoder viz. Convolutional autoencoder, Variational autoencoder, Sparse autoencoder, stacked autoencoder, Deep autoencoder, to name a few, have been thoroughly studied. How the layer size and depth of deep autoencoder model affect the overall performance of the system has also been discussed. We also outlined the suitability of various autoencoder architectures to different application areas. This would help the research community to choose the suitable autoencoder architecture for the problem to be solved.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Autoencoders,Data representation,Deep learning,Dimensionality reduction,Representation learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\JZXABSFU\\Pawar und Attar - 2020 - Assessment of Autoencoder Architectures for Data R.pdf}
}

@inproceedings{peiAutomatedProgramRepair2015,
  title = {Automated Program Repair in an Integrated Development Environment},
  booktitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  author = {Pei, Y. and Furia, C. A. and Nordio, M. and Meyer, B.},
  date = {2015-05},
  volume = {2},
  pages = {681--684},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2015.222},
  abstract = {We present the integration of the AutoFix automated program repair technique into the EiffelStudio Development Environment. AutoFix presents itself like a recommendation system capable of automatically finding bugs and suggesting fixes in the form of source-code patches. Its performance suggests usage scenarios where it runs in the background or during work interruptions, displaying fix suggestions as they become available. This is a contribution towards the vision of semantic Integrated Development Environments, which offer powerful automated functionality within interfaces familiar to developers. A screencast highlighting the main features of AutoFix can be found at: http://youtu.be/Ff2ULiyL-80.},
  eventtitle = {2015 {{IEEE}}/{{ACM}} 37th {{IEEE International Conference}} on {{Software Engineering}}},
  keywords = {Algorithm design and analysis,AutoFix automated program repair technique,Computer bugs,Contracts,EiffelStudio development environment,Heuristic algorithms,Maintenance engineering,program diagnostics,recommendation system,semantic integrated development environments,Semantics,software maintenance,source code (software),source-code patches,Testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\9J225FWG\\Pei et al. - 2015 - Automated Program Repair in an Integrated Developm.pdf;C\:\\Users\\felix\\Zotero\\storage\\VBGQEHKU\\7203042.html}
}

@inproceedings{pezzeAutomaticGUITesting2018,
  title = {Automatic {{GUI}} Testing of Desktop Applications: An Empirical Assessment of the State of the Art},
  shorttitle = {Automatic {{GUI}} Testing of Desktop Applications},
  booktitle = {Companion {{Proceedings}} for the {{ISSTA}}/{{ECOOP}} 2018 {{Workshops}}},
  author = {Pezzè, Mauro and Rondena, Paolo and Zuddas, Daniele},
  date = {2018-07-16},
  series = {{{ISSTA}} '18},
  pages = {54--62},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3236454.3236489},
  abstract = {Testing software applications interacting with their graphical user interface, in short GUI testing, is both important, since it can reveal subtle and annoying bugs, and expensive, due to myriads of possible GUI interactions. Recent attempts to automate GUI testing have produced several techniques that address the problem from different perspectives, sometimes focusing only on some specific platforms, such as Android or Web, and sometimes targeting only some aspects of GUI testing, like test case generation or execution. Although GUI test case generation techniques for desktop applications were the first to be investigated, this area is still actively researched and its state of the art is continuously expanding. In this paper we comparatively evaluate the state-of-the-art for automatic GUI test cases generation for desktop applications, by presenting a set of experimental results obtained with the main GUI testing tools for desktop applications available. The paper overviews the state of the art in GUI testing, discusses differences, similarities and complementarities among the different techniques, experimentally compares strengths and weaknesses, and pinpoints the open problems that deserve further investigation.},
  keywords = {automatic test case generation,empirical comparison,GUI testing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CC8XFTNV\\Pezzè et al. - 2018 - Automatic GUI testing of desktop applications an .pdf}
}

@software{PistonDevelopersConrod2014,
  title = {{{PistonDevelopers}}/Conrod},
  origdate = {2014-06-14T10:25:10Z},
  url = {https://github.com/PistonDevelopers/conrod},
  urldate = {2021-12-13},
  abstract = {An easy-to-use, 2D GUI library written entirely in Rust.},
  organization = {{PistonDevelopers}},
  keywords = {gui,rust}
}

@inproceedings{ponzanelliMiningStackOverflowTurn2014,
  title = {Mining {{StackOverflow}} to Turn the {{IDE}} into a Self-Confident Programming Prompter},
  booktitle = {Proceedings of the 11th {{Working Conference}} on {{Mining Software Repositories}}},
  author = {Ponzanelli, Luca and Bavota, Gabriele and Di Penta, Massimiliano and Oliveto, Rocco and Lanza, Michele},
  date = {2014-05-31},
  series = {{{MSR}} 2014},
  pages = {102--111},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2597073.2597077},
  abstract = {Developers often require knowledge beyond the one they possess, which often boils down to consulting sources of information like Application Programming Interfaces (API) documentation, forums, Q\&A websites, etc. Knowing what to search for and how is non- trivial, and developers spend time and energy to formulate their problems as queries and to peruse and process the results. We propose a novel approach that, given a context in the IDE, automatically retrieves pertinent discussions from Stack Overflow, evaluates their relevance, and, if a given confidence threshold is surpassed, notifies the developer about the available help. We have implemented our approach in Prompter, an Eclipse plug-in. Prompter has been evaluated through two studies. The first was aimed at evaluating the devised ranking model, while the second was conducted to evaluate the usefulness of Prompter.},
  isbn = {978-1-4503-2863-0},
  keywords = {Developers Support,Empirical Studies,Recommender Systems},
  file = {C\:\\Users\\felix\\Zotero\\storage\\LC9Z2WM2\\Ponzanelli et al. - 2014 - Mining StackOverflow to turn the IDE into a self-c.pdf}
}

@inproceedings{ponzanelliPrompterSelfconfidentRecommender2014,
  title = {Prompter: A Self-Confident Recommender System},
  shorttitle = {Prompter},
  booktitle = {2014 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}},
  author = {Ponzanelli, L. and Bavota, G. and Penta, M. D. and Oliveto, R. and Lanza, M.},
  date = {2014-09},
  pages = {577--580},
  issn = {1063-6773},
  doi = {10.1109/ICSME.2014.99},
  abstract = {Developers often consult different sources of information like Application Programming Interfaces (API) documentation, forums, Q\&A websites, etc. With the aim of gathering additional knowledge for the programming task at hand. The process of searching and identifying valuable pieces of information requires developers to spend time and energy in formulating the right queries, assessing the returned results, and integrating the obtained knowledge into the code base. All of this is often done manually. We present Prompter, a plug-in for the Eclipse IDE which automatically searches and identifies relevant Stack Overflow discussions, evaluates their relevance given the code context in the IDE, and notifies the developer if and only if a user-defined confidence threshold is surpassed.},
  eventtitle = {2014 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}}},
  keywords = {code base,Context,Context modeling,Eclipse IDE,Observers,Programming,programming task,Prompter,query processing,recommender systems,Recommender systems,self-confident recommender system,Servers,stack overflow discussions,user-defined confidence threshold,Writing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\JXDBRZXT\\Ponzanelli et al. - 2014 - Prompter A Self-Confident Recommender System.pdf;C\:\\Users\\felix\\Zotero\\storage\\XJWPCM8C\\6976143.html}
}

@inproceedings{ponzanelliSupportingSoftwareDevelopers2017,
  title = {Supporting Software Developers with a Holistic Recommender System},
  booktitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Ponzanelli, L. and Scalabrino, S. and Bavota, G. and Mocci, A. and Oliveto, R. and Penta, M. Di and Lanza, M.},
  date = {2017-05},
  pages = {94--105},
  issn = {1558-1225},
  doi = {10.1109/ICSE.2017.17},
  abstract = {The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q\&A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development.},
  eventtitle = {2017 {{IEEE}}/{{ACM}} 39th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  keywords = {Browsers,heterogeneous resources,holistic meta-information model,holistic recommender system,information analysis,information context analysis,information navigation,information retrieval,information search,intelligent support,interactive navigation chart,interactive programming,Libra,Mining unstructured data,Navigation,online front-ends,pertinent Q&A pages,program entities,programming tasks,recommender systems,Recommender systems,software developers,software development,software engineering,Uniform resource locators,User interfaces,Web browser,Web pages,Web search},
  file = {C\:\\Users\\felix\\Zotero\\storage\\S3VUGNU9\\Ponzanelli et al. - 2017 - Supporting Software Developers with a Holistic Rec.pdf;C\:\\Users\\felix\\Zotero\\storage\\7RYCWB77\\7985653.html}
}

@software{ProcgenAutoencoders2021,
  title = {Procgen {{Autoencoders}}},
  date = {2021-08-02T12:42:05Z},
  origdate = {2021-01-12T12:34:47Z},
  url = {https://github.com/neuroevolution-ai/ProcgenAutoencoder},
  urldate = {2021-09-03},
  organization = {{NeuroEvolution A.I.}}
}

@inproceedings{prokschEnrichingInIDEProcess2017,
  title = {Enriching In-{{IDE}} Process Information with Fine-Grained Source Code History},
  booktitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  author = {Proksch, S. and Nadi, S. and Amann, S. and Mezini, M.},
  date = {2017-02},
  pages = {250--260},
  doi = {10.1109/SANER.2017.7884626},
  abstract = {Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CA□RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CA□RET and FEEDBAG++.},
  eventtitle = {2017 {{IEEE}} 24th {{International Conference}} on {{Software Analysis}}, {{Evolution}} and {{Reengineering}} ({{SANER}})},
  keywords = {CA□RET,code snapshots,Cognition,Context,context information,FEEDBAG++,fine-grained source code history,History,in-IDE process information,Instruments,programming environments,simplified syntax trees,Software,software development,software tools,source code (software),SSTs,Syntactics,version-control systems,visual studio instrumentation,Visualization},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DS2URF3T\\Proksch et al. - 2017 - Enriching in-IDE process information with fine-gra.pdf;C\:\\Users\\felix\\Zotero\\storage\\HDUSPYPB\\7884626.html}
}

@inproceedings{prokschEvaluatingEvaluationsCode2016,
  title = {Evaluating the Evaluations of Code Recommender Systems: A Reality Check},
  shorttitle = {Evaluating the Evaluations of Code Recommender Systems},
  booktitle = {2016 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  author = {Proksch, S. and Amann, S. and Nadi, S. and Mezini, M.},
  date = {2016-09},
  pages = {111--121},
  abstract = {While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reflect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reflected in artificial evaluations.},
  eventtitle = {2016 31st {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}} ({{ASE}})},
  keywords = {Artificial Evaluation,Benchmark testing,code recommender system,code search,code-snippet completion,Context,Empirical Study,History,IDE Interaction Data,method-call completion,MIMICs,program testing,Proposals,Recommender systems,Software,software maintenance},
  file = {C\:\\Users\\felix\\Zotero\\storage\\Y2675HER\\Proksch et al. - 2016 - Evaluating the evaluations of code recommender sys.pdf;C\:\\Users\\felix\\Zotero\\storage\\CIN7JB46\\7582750.html}
}

@online{QtWidgetsCrates,
  title = {Qt\_widgets - Crates.Io: {{Rust Package Registry}}},
  url = {https://crates.io/crates/qt_widgets/0.5.0},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\7QC98GLG\\0.5.html}
}

@software{ramonHecrjIced2019,
  title = {Hecrj/Iced},
  author = {Ramón, Héctor},
  origdate = {2019-07-15T22:34:46Z},
  url = {https://github.com/hecrj/iced},
  urldate = {2021-12-13},
  abstract = {A cross-platform GUI library for Rust, inspired by Elm},
  keywords = {elm,graphics,gui,interface,renderer-agnostic,rust,toolkit,user-interface,widget,widgets}
}

@software{ramonHecrjIced2019a,
  title = {Hecrj/Iced},
  author = {Ramón, Héctor},
  origdate = {2019-07-15T22:34:46Z},
  url = {https://github.com/hecrj/iced},
  urldate = {2021-12-13},
  abstract = {A cross-platform GUI library for Rust, inspired by Elm}
}

@inproceedings{ranzatoUnsupervisedLearningInvariant2007,
  title = {Unsupervised {{Learning}} of {{Invariant Feature Hierarchies}} with {{Applications}} to {{Object Recognition}}},
  booktitle = {2007 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann},
  date = {2007-06},
  pages = {1--8},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2007.383157},
  abstract = {We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64\% error on MNIST, and 54\% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.},
  eventtitle = {2007 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Computer architecture,Computer vision,Convolution,Detectors,Feature extraction,Gabor filters,Object detection,Object recognition,Supervised learning,Unsupervised learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\E9ZCWQEZ\\Ranzato et al. - 2007 - Unsupervised Learning of Invariant Feature Hierarc.pdf;C\:\\Users\\felix\\Zotero\\storage\\JIUYGASZ\\4270182.html}
}

@inproceedings{raychevCodeCompletionStatistical2014,
  title = {Code Completion with Statistical Language Models},
  booktitle = {Proceedings of the 35th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
  date = {2014-06-09},
  series = {{{PLDI}} '14},
  pages = {419--428},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2594291.2594321},
  abstract = {We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls. Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments. Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90\% of the cases.},
  isbn = {978-1-4503-2784-8},
  file = {C\:\\Users\\felix\\Zotero\\storage\\Z73VTIRP\\Raychev et al. - 2014 - Code completion with statistical language models.pdf}
}

@software{RedoxosOrbtk2015,
  title = {Redox-Os/Orbtk},
  origdate = {2015-12-14T19:17:13Z},
  url = {https://github.com/redox-os/orbtk},
  urldate = {2021-12-13},
  abstract = {The Rust UI-Toolkit.},
  organization = {{Redox OS}},
  keywords = {cross-platform,entity-component,gui,redox-os,rust,rust-lang,widgets}
}

@software{RedoxosOrbtk2021a,
  title = {Redox-Os/Orbtk},
  date = {2021-07-31T18:57:51Z},
  origdate = {2015-12-14T19:17:13Z},
  url = {https://github.com/redox-os/orbtk},
  urldate = {2021-08-01},
  abstract = {The Rust UI-Toolkit.},
  organization = {{Redox OS}},
  keywords = {cross-platform,entity-component,gui,redox-os,rust,rust-lang,widgets}
}

@inproceedings{ricciIntroductionRecommenderSystems2011,
  title = {Introduction to Recommender Systems Handbook},
  booktitle = {Recommender {{Systems Handbook}}},
  author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha and Kantor, Paul B.},
  date = {2011},
  pages = {1--35},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-85820-3_1},
  abstract = {Recommender Systems (RSs) are software tools and techniques providing suggestions for items to be of use to a user. In this introductory chapter we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters included in this handbook and to help the reader navigate the extremely rich and detailed content that the handbook offers.},
  isbn = {978-0-387-85820-3},
  langid = {english},
  keywords = {Mender System,Recommendation Algorithm,Recommendation List,Recommendation Process,Recommender System},
  file = {C\:\\Users\\felix\\Zotero\\storage\\WXXUYEP2\\Ricci et al. - 2011 - Introduction to Recommender Systems Handbook.pdf}
}

@inproceedings{ricciRecommenderSystemsIntroduction2015,
  title = {Recommender Systems: Introduction and Challenges},
  shorttitle = {Recommender Systems},
  booktitle = {Recommender {{Systems Handbook}}},
  author = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
  editor = {Ricci, Francesco and Rokach, Lior and Shapira, Bracha},
  date = {2015},
  pages = {1--34},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-1-4899-7637-6_1},
  abstract = {Recommender Systems (RSs) are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user. In this introductory chapter, we briefly discuss basic RS ideas and concepts. Our main goal is to delineate, in a coherent and structured way, the chapters included in this handbook. Additionally, we aim to help the reader navigate the rich and detailed content that this handbook offers.},
  isbn = {978-1-4899-7637-6},
  langid = {english},
  keywords = {Normalize Discount Cumulative Gain,Recommendation Algorithm,Recommendation List,Recommendation Technique,Recommender System},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CTU7BTWX\\Ricci et al. - 2015 - Recommender Systems Introduction and Challenges.pdf}
}

@article{robillardRecommendationSystemsSoftware2010,
  title = {Recommendation Systems for Software Engineering},
  author = {Robillard, M. and Walker, R. and Zimmermann, T.},
  date = {2010-07},
  journaltitle = {IEEE Software},
  volume = {27},
  number = {4},
  pages = {80--86},
  issn = {1937-4194},
  doi = {10.1109/MS.2009.161},
  abstract = {Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.},
  eventtitle = {{{IEEE Software}}},
  keywords = {bug reports,coding tools and techniques,design tools and techniques,development tools,information space,Navigation,Programming,programming environments,recommendation system,recommender systems,software construction tools,software development,software engineering,Software engineering,software tool,software tools,Software tools,time seeking information,value-producing task,Writing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\9IY8PAP2\\Robillard et al. - 2010 - Recommendation Systems for Software Engineering.pdf;C\:\\Users\\felix\\Zotero\\storage\\7UDN5MCF\\5235134.html}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  date = {1958},
  journaltitle = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471(Electronic),0033-295X(Print)},
  doi = {10.1037/h0042519},
  abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Brain,Cognition,Memory,Nervous System},
  file = {C\:\\Users\\felix\\Zotero\\storage\\KPTQMGMP\\1959-09865-001.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5RQVA3H7\\Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;C\:\\Users\\felix\\Zotero\\storage\\VXHIKWR6\\323533a0.html}
}

@software{RustqtRitual2016,
  title = {Rust-Qt/Ritual},
  origdate = {2016-04-28T18:15:59Z},
  url = {https://github.com/rust-qt/ritual},
  urldate = {2021-12-13},
  abstract = {Use C++ libraries from Rust},
  organization = {{Rust-Qt}},
  keywords = {cpp,cpp-bindings,crates,qt,qt-crate,qt-generator,rituals,rust,rust-bindings,rust-crate,rust-generator}
}

@online{RustRuntimeRust,
  title = {The {{Rust}} Runtime - {{The Rust Reference}}},
  url = {https://doc.rust-lang.org/reference/runtime.html},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\D43TIPPV\\runtime.html}
}

@software{schuttFschuttAzul2018,
  title = {Fschutt/Azul},
  author = {Schütt, Felix},
  origdate = {2018-01-16T20:57:07Z},
  url = {https://github.com/fschutt/azul},
  urldate = {2021-12-13},
  abstract = {Desktop GUI Framework},
  keywords = {c,cpp,desktop,desktop-gui-framework,gui,gui-framework,gui-library,opengl,rust}
}

@software{ScitersdkRustsciter2016,
  title = {Sciter-Sdk/Rust-Sciter},
  origdate = {2016-03-07T08:02:47Z},
  url = {https://github.com/sciter-sdk/rust-sciter},
  urldate = {2021-12-13},
  abstract = {Rust bindings for Sciter},
  organization = {{Terra Informatica Software, Inc}},
  keywords = {gui,htmlayout,rust-bindings,sciter,tiscript}
}

@software{ServoCorefoundationrs2012,
  title = {Servo/Core-Foundation-Rs},
  origdate = {2012-08-30T21:58:23Z},
  url = {https://github.com/servo/core-foundation-rs},
  urldate = {2021-12-13},
  abstract = {Rust bindings to Core Foundation and other low level libraries on Mac OS X and iOS},
  organization = {{Servo}}
}

@software{ServoWebrender2015,
  title = {Servo/Webrender},
  origdate = {2015-09-21T21:27:30Z},
  url = {https://github.com/servo/webrender},
  urldate = {2021-12-13},
  abstract = {A GPU-based renderer for the web},
  organization = {{Servo}}
}

@software{SixtyfpsuiSixtyfps2020,
  title = {Sixtyfpsui/Sixtyfps},
  origdate = {2020-05-04T08:53:03Z},
  url = {https://github.com/sixtyfpsui/sixtyfps},
  urldate = {2021-12-13},
  abstract = {SixtyFPS is a toolkit to efficiently develop fluid graphical user interfaces for any display: embedded devices and desktop applications. We support multiple programming languages, such as Rust, C++ or JavaScript.},
  organization = {{SixtyFPS}},
  keywords = {desktop-applications,embedded-devices,gui,language,rust,wasm}
}

@software{skylotSkylotJadx2013,
  title = {Skylot/Jadx},
  author = {{skylot}},
  origdate = {2013-03-18T17:08:21Z},
  url = {https://github.com/skylot/jadx},
  urldate = {2021-12-13},
  abstract = {Dex to Java decompiler. Contribute to skylot/jadx development by creating an account on GitHub.},
  keywords = {android,decompiler,dex,java}
}

@online{stevewhimsRenderTargetsOverview,
  title = {Render {{Targets Overview}} - {{Win32}} Apps},
  author = {{stevewhims}},
  url = {https://docs.microsoft.com/en-us/windows/win32/direct2d/render-targets-overview},
  urldate = {2021-12-13},
  abstract = {Describes the different types of Direct2D render targets and how to use them.},
  langid = {american},
  file = {C\:\\Users\\felix\\Zotero\\storage\\XZU3AAQS\\render-targets-overview.html}
}

@software{stokkeBodilVgtk2019,
  title = {Bodil/Vgtk},
  author = {Stokke, Bodil},
  origdate = {2019-01-06T03:12:19Z},
  url = {https://github.com/bodil/vgtk},
  urldate = {2021-12-13},
  abstract = {A declarative desktop UI framework for Rust built on GTK and Gtk-rs}
}

@software{TauriappsTauri2019,
  title = {Tauri-Apps/Tauri},
  origdate = {2019-07-13T09:09:37Z},
  url = {https://github.com/tauri-apps/tauri},
  urldate = {2021-12-13},
  abstract = {Build smaller, faster, and more secure desktop applications with a web frontend.},
  organization = {{Tauri}},
  keywords = {hacktoberfest,high-performance,rust,webview,works-with-clojurescript,works-with-construct,works-with-elm,works-with-flutter,works-with-gatsby,works-with-mint,works-with-phaser,works-with-quasar,works-with-react,works-with-reason,works-with-svelte,works-with-vue,works-with-yew}
}

@article{terraJMoveNovelHeuristic2018,
  title = {{{JMove}}: A Novel Heuristic and Tool to Detect Move Method Refactoring Opportunities},
  shorttitle = {{{JMove}}},
  author = {Terra, Ricardo and Valente, Marco Tulio and Miranda, Sergio and Sales, Vitor},
  date = {2018-04},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {138},
  pages = {19--36},
  issn = {01641212},
  doi = {10.1016/j.jss.2017.11.073},
  abstract = {This paper presents a recommendation approach that suggests Move Method refactorings using the static dependencies established by methods. This approach, implemented in a publicly available tool called JMove, compares the similarity of the dependencies established by a method with the dependencies established by the methods in possible target classes. We first evaluate JMove using 195 Move Method refactoring opportunities, synthesized in 10 open-source systems. In this evaluation, JMove precision ranges from 21\% (small methods) to 32\% (large methods) and its median recall ranges from 21\% (small methods) to 60\% (large methods). In the same scenario, JDeodorant, which is a state-of-the-art Move Method recommender, has a maximal precision of 15\% (large methods) and a maximal median recall of 40\% (small methods). Therefore, we claim that JMove is specially useful to provide recommendations for large methods. We reinforce this claim by means of two other studies. First, by investigating the overlapping of the recommendations provided by JMove and three other recommenders (JDeodorant, inCode, and Methodbook). Second, by validating JMove and JDeodorant recommendations with experts in two industrial-strength systems.},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\B9D3NHEN\\2018-jss-jmove.pdf}
}

@article{terraJMoveNovelHeuristic2018a,
  title = {{{JMove}}: A Novel Heuristic and Tool to Detect Move Method Refactoring Opportunities},
  shorttitle = {{{JMove}}},
  author = {Terra, Ricardo and Valente, Marco Tulio and Miranda, Sergio and Sales, Vitor},
  date = {2018-04-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {138},
  pages = {19--36},
  publisher = {{Elsevier}},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.11.073},
  abstract = {This paper presents a recommendation approach that suggests Move Method refactorings using the static dependencies established by methods. This approach, implemented in a publicly available tool called JMove, compares the similarity of the dependencies established by a method with the dependencies established by the methods in possible target classes. We first evaluate JMove using 195 Move Method refactoring opportunities, synthesized in 10 open-source systems. In this evaluation, JMove precision ranges from 21\% (small methods) to 32\% (large methods) and its median recall ranges from 21\% (small methods) to 60\% (large methods). In the same scenario, JDeodorant, which is a state-of-the-art Move Method recommender, has a maximal precision of 15\% (large methods) and a maximal median recall of 40\% (small methods). Therefore, we claim that JMove is specially useful to provide recommendations for large methods. We reinforce this claim by means of two other studies. First, by investigating the overlapping of the recommendations provided by JMove and three other recommenders (JDeodorant, inCode, and Methodbook). Second, by validating JMove and JDeodorant recommendations with experts in two industrial-strength systems.},
  langid = {english},
  keywords = {Dependency sets,JDeodorant,JMove,Methodbook,Move method refactorings,Recommendation systems},
  file = {C\:\\Users\\felix\\Zotero\\storage\\DLJ2M9C8\\Terra et al. - 2018 - JMove A novel heuristic and tool to detect move m.pdf;C\:\\Users\\felix\\Zotero\\storage\\A6FHKF4M\\S0164121217302960.html}
}

@article{terraJMoveNovelHeuristic2018b,
  title = {{{JMove}}: {{A}} Novel Heuristic and Tool to Detect Move Method Refactoring Opportunities},
  author = {Terra, Ricardo and Valente, Marco Tulio and Miranda, Sergio and Sales, Vitor},
  date = {2018},
  journaltitle = {Journal of Systems and Software},
  volume = {138},
  pages = {19--36},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2017.11.073},
  abstract = {This paper presents a recommendation approach that suggests Move Method refactorings using the static dependencies established by methods. This approach, implemented in a publicly available tool called JMove, compares the similarity of the dependencies established by a method with the dependencies established by the methods in possible target classes. We first evaluate JMove using 195 Move Method refactoring opportunities, synthesized in 10 open-source systems. In this evaluation, JMove precision ranges from 21\% (small methods) to 32\% (large methods) and its median recall ranges from 21\% (small methods) to 60\% (large methods). In the same scenario, JDeodorant, which is a state-of-the-art Move Method recommender, has a maximal precision of 15\% (large methods) and a maximal median recall of 40\% (small methods). Therefore, we claim that JMove is specially useful to provide recommendations for large methods. We reinforce this claim by means of two other studies. First, by investigating the overlapping of the recommendations provided by JMove and three other recommenders (JDeodorant, inCode, and Methodbook). Second, by validating JMove and JDeodorant recommendations with experts in two industrial-strength systems.},
  keywords = {Dependency sets,JDeodorant,JMove,Methodbook,Move method refactorings,Recommendation systems}
}

@article{terraJMoveNovelHeuristic2018c,
  title = {{{JMove}}: {{A}} Novel Heuristic and Tool to Detect Move Method Refactoring Opportunities},
  author = {Terra, Ricardo and Valente, Marco Tulio and Miranda, Sergio and Sales, Vitor},
  date = {2018},
  journaltitle = {Journal of Systems and Software},
  volume = {138},
  pages = {19--36},
  publisher = {{Elsevier}}
}

@software{tindallNoraCodesLibuirs2019,
  title = {{{NoraCodes}}/Libui-Rs},
  author = {Tindall, Leonora},
  origdate = {2019-01-22T22:33:31Z},
  url = {https://github.com/NoraCodes/libui-rs},
  urldate = {2021-12-13},
  abstract = {Rust bindings to the minimalist, native, cross-platform UI toolkit `libui`}
}

@article{tsantalisIdentificationMoveMethod2009,
  title = {Identification of Move Method Refactoring Opportunities},
  author = {Tsantalis, N. and Chatzigeorgiou, A.},
  date = {2009-05},
  journaltitle = {IEEE Transactions on Software Engineering},
  volume = {35},
  number = {3},
  pages = {347--367},
  issn = {1939-3520},
  doi = {10.1109/TSE.2009.1},
  abstract = {Placement of attributes/methods within classes in an object-oriented system is usually guided by conceptual criteria and aided by appropriate metrics. Moving state and behavior between classes can help reduce coupling and increase cohesion, but it is nontrivial to identify where such refactorings should be applied. In this paper, we propose a methodology for the identification of Move Method refactoring opportunities that constitute a way for solving many common feature envy bad smells. An algorithm that employs the notion of distance between system entities (attributes/methods) and classes extracts a list of behavior-preserving refactorings based on the examination of a set of preconditions. In practice, a software system may exhibit such problems in many different places. Therefore, our approach measures the effect of all refactoring suggestions based on a novel entity placement metric that quantifies how well entities have been placed in system classes. The proposed methodology can be regarded as a semi-automatic approach since the designer will eventually decide whether a suggested refactoring should be applied or not based on conceptual or other design quality criteria. The evaluation of the proposed approach has been performed considering qualitative, metric, conceptual, and efficiency aspects of the suggested refactorings in a number of open-source projects.},
  keywords = {conceptual criteria,Data mining,design quality.,feature envy,Feature Envy,Jaccard distance,Move Method refactoring,Move Method refactoring opportunity identification,object-oriented design,object-oriented programming,object-oriented system,Open source software,Performance evaluation,Productivity,Runtime,software maintenance,software metrics,Software systems},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5WL5PCZ2\\Tsantalis und Chatzigeorgiou - 2009 - Identification of Move Method Refactoring Opportun.pdf;C\:\\Users\\felix\\Zotero\\storage\\9G3NW36T\\4752842.html}
}

@incollection{vogelArchitekturVorgehenWIE2009,
  title = {Architektur-Vorgehen (WIE)},
  booktitle = {Software-Architektur: Grundlagen — Konzepte — Praxis},
  editor = {Vogel, Oliver and Arnold, Ingo and Chughtai, Arif and Ihler, Edmund and Kehrer, Timo and Mehlig, Uwe and Zdun, Uwe},
  date = {2009},
  pages = {341--448},
  publisher = {{Spektrum Akademischer Verlag}},
  location = {{Heidelberg}},
  doi = {10.1007/978-3-8274-2267-5_8},
  abstract = {In diesem Kapitel steht die WIE-Dimension des Ordnungsrahmens im Mittelpunkt. Zunächst wird für einen Architekten relevantes Wissen zu Entwicklungsprozessen vermittelt. Darauf aufbauend werden die einzelnen Tätigkeiten eines Architekten während der Erarbeitung eines Systems auf einem allgemeingültigen Niveau beschrieben. Abschließend erfolgt eine Konkretisierung dieser Darstellung anhand eines beispielhaften Anwendungsszenarios. Das Anwendungsszenario vernetzt den Ordnungsrahmen sowie den Theorieteil aus dem Kontext eines spezifischen Anwendungsfalls heraus und bietet auf diese Weise dem Leser einen problemorientierten Zugang zu den übrigen Kapiteln.},
  isbn = {978-3-8274-2267-5},
  langid = {german},
  file = {C\:\\Users\\felix\\Zotero\\storage\\5LRAELNE\\Vogel et al. - 2009 - Architektur-Vorgehen (WIE).pdf}
}

@online{WebGPU,
  title = {{{WebGPU}}},
  url = {https://www.w3.org/TR/webgpu/},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\JC73MXBF\\webgpu.html}
}

@online{WebGPUa,
  title = {{{WebGPU}}},
  url = {https://www.w3.org/TR/webgpu/},
  urldate = {2021-07-12}
}

@online{WebGPUb,
  title = {{{WebGPU}}},
  url = {https://www.w3.org/TR/webgpu/},
  urldate = {2021-07-12}
}

@online{WebGPUc,
  title = {{{WebGPU}}},
  url = {https://www.w3.org/TR/webgpu/},
  urldate = {2021-07-12}
}

@online{WhatOwnershipRust,
  title = {What Is {{Ownership}}? - {{The Rust Programming Language}}},
  url = {https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html},
  urldate = {2021-12-13},
  file = {C\:\\Users\\felix\\Zotero\\storage\\W7EW5EMM\\ch04-01-what-is-ownership.html}
}

@software{WoboqQmetaobjectrs2018,
  title = {Woboq/Qmetaobject-Rs},
  origdate = {2018-06-06T12:32:41Z},
  url = {https://github.com/woboq/qmetaobject-rs},
  urldate = {2021-12-13},
  abstract = {Integrate Qml and Rust by building the QMetaObject at compile time.},
  organization = {{Woboq GmbH}}
}

@incollection{yeDeepDiveKeras2022,
  title = {A {{Deep Dive}} into {{Keras}}},
  booktitle = {Modern {{Deep Learning Design}} and {{Application Development}}: {{Versatile Tools}} to {{Solve Deep Learning Problems}}},
  author = {Ye, Andre},
  editor = {Ye, Andre},
  date = {2022},
  pages = {1--48},
  publisher = {{Apress}},
  location = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-7413-2_1},
  abstract = {One of the key goals of this book is to explore concepts in deep learning as an analytical art – the computing environment is your canvas, and deep learning design is your brush. A strong grasp of analytical creativity and imagination is integral to understanding deep learning at an instinctive, visceral level. This book aims to provide you with that understanding. As the world of deep learning constantly changes, those that attach their knowledge of deep learning to only the tools of today will find it difficult to work with deep learning when new syntaxes and libraries are introduced. An intuitive sense of playfulness and intellectual freedom and curiosity, though, can get us closer to working with deep learning closely, personally, and successfully beyond an interdependence on any one specific model architecture or framework.},
  isbn = {978-1-4842-7413-2},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\8ZV5J93S\\Ye - 2022 - A Deep Dive into Keras.pdf}
}

@book{yeModernDeepLearning2022,
  title = {Modern {{Deep Learning Design}} and {{Application Development}}: {{Versatile Tools}} to {{Solve Deep Learning Problems}}},
  shorttitle = {Modern {{Deep Learning Design}} and {{Application Development}}},
  author = {Ye, Andre},
  date = {2022},
  publisher = {{Apress}},
  location = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-7413-2},
  isbn = {978-1-4842-7412-5 978-1-4842-7413-2},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\JIWS7V6Z\\Ye - 2022 - Modern Deep Learning Design and Application Develo.pdf}
}

@incollection{yeVersatilityAutoencoders2022,
  title = {The {{Versatility}} of {{Autoencoders}}},
  booktitle = {Modern {{Deep Learning Design}} and {{Application Development}}: {{Versatile Tools}} to {{Solve Deep Learning Problems}}},
  author = {Ye, Andre},
  editor = {Ye, Andre},
  date = {2022},
  pages = {115--203},
  publisher = {{Apress}},
  location = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-7413-2_3},
  abstract = {The concept of encoding and decoding entities – ideas, images, physical material, information, and so on – is a particularly profound and important one, because it is so deeply embedded into how we experience and understand the environment around us. Encoding and decoding information is a key process in communication and learning. Every time you communicate with someone, observe the weather, read from a book – like this one – or in some way interact with information, you are engaging in a process of encoding and decoding, observing and interpreting. We can make use of this idea with the deep learning concept of an autoencoder.},
  isbn = {978-1-4842-7413-2},
  langid = {english},
  file = {C\:\\Users\\felix\\Zotero\\storage\\HCE4MGI4\\Ye - 2022 - The Versatility of Autoencoders.pdf}
}

@inproceedings{zhongMAPOMiningRecommending2009,
  title = {{{MAPO}}: Mining and Recommending {{API}} Usage Patterns},
  shorttitle = {Mapo},
  booktitle = {{{ECOOP}} 2009 – {{Object-Oriented Programming}}},
  author = {Zhong, Hao and Xie, Tao and Zhang, Lu and Pei, Jian and Mei, Hong},
  editor = {Drossopoulou, Sophia},
  date = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {318--343},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-03013-0_15},
  abstract = {To improve software productivity, when constructing new software systems, programmers often reuse existing libraries or frameworks by invoking methods provided in their APIs. Those API methods, however, are often complex and not well documented. To get familiar with how those API methods are used, programmers often exploit a source code search tool to search for code snippets that use the API methods of interest. However, the returned code snippets are often large in number, and the huge number of snippets places a barrier for programmers to locate useful ones. In order to help programmers overcome this barrier, we have developed an API usage mining framework and its supporting tool called MAPO (Mining API usage Pattern from Open source repositories) for mining API usage patterns automatically. A mined pattern describes that in a certain usage scenario, some API methods are frequently called together and their usages follow some sequential rules. MAPO further recommends the mined API usage patterns and their associated code snippets upon programmers’ requests. Our experimental results show that with these patterns MAPO helps programmers locate useful code snippets more effectively than two state-of-the-art code search tools. To investigate whether MAPO can assist programmers in programming tasks, we further conducted an empirical study. The results show that using MAPO, programmers produce code with fewer bugs when facing relatively complex API usages, comparing with using the two state-of-the-art code search tools.},
  isbn = {978-3-642-03013-0},
  langid = {english},
  keywords = {Call Sequence,Code Snippet,Frequent Sequence,Open Source Project,Programming Task},
  file = {C\:\\Users\\felix\\Zotero\\storage\\CDZWX4U7\\Zhong et al. - 2009 - MAPO Mining and Recommending API Usage Patterns.pdf}
}

@incollection{zhuDeepLearningLearning2020,
  title = {Deep {{Learning}} for {{Learning Graph Representations}}},
  booktitle = {Deep {{Learning}}: {{Concepts}} and {{Architectures}}},
  author = {Zhu, Wenwu and Wang, Xin and Cui, Peng},
  editor = {Pedrycz, Witold and Chen, Shyi-Ming},
  date = {2020},
  series = {Studies in {{Computational Intelligence}}},
  pages = {169--210},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-31756-0_6},
  abstract = {Mining graph data has become a popular research topic in computer science and has been widely studied in both academia and industry given the increasing amount of network data in the recent years. However, the huge amount of network data has posed great challenges for efficient analysis. This motivates the advent of graph representation which maps the graph into a low-dimension vector space, keeping original graph structure and supporting graph inference. The investigation on efficient representation of a graph has profound theoretical significance and important realistic meaning, we therefore introduce some basic ideas in graph representation/network embedding as well as some representative models in this chapter.},
  isbn = {978-3-030-31756-0},
  langid = {english},
  keywords = {Deep learning,Graph representation,Network embedding},
  file = {C\:\\Users\\felix\\Zotero\\storage\\K5HSSS77\\Zhu et al. - 2020 - Deep Learning for Learning Graph Representations.pdf}
}

@online{zotero-245,
  url = {https://scholar.googleusercontent.com/scholar.bib?q=info:_mHrLfm2BQAJ:scholar.google.com/&output=citation&scisdr=CgWDt0mkEPG_lCGyLE8:AAGBfm0AAAAAYKO0NE8-pp1Hca0ANG6I0Xw-5_e1GS8s&scisig=AAGBfm0AAAAAYKO0NLAaQ1T-isU8Q6i24bxsb0buae1e&scisf=4&ct=citation&cd=-1&hl=de},
  urldate = {2021-05-18},
  file = {C\:\\Users\\felix\\Zotero\\storage\\7DWS48BA\\scholar.html}
}


